{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code extracts a lot of features for iterative model training purposes, but for our final model, we are only interested in the 23ms, which is the default hop_length. So, you can just not specify the hop_length argument, then you can extract the audio features accordingly. Remember to average it later on. For the working product, there will be no need to store in a h5 or csv file, you may just store the extracted features into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Extraction\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from moviepy.editor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hop_length(sr, ms):\n",
    "    return int(sr * ms / 1000)\n",
    "\n",
    "\n",
    "def extract_features_for_different_timesteps(y, sr):\n",
    "    timesteps = [23, 100, 500, 1000]  # in milliseconds\n",
    "    features_dict = {}\n",
    "\n",
    "    for ms in timesteps:\n",
    "        hop_length = calculate_hop_length(sr, ms)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=hop_length)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=hop_length)\n",
    "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr, hop_length=hop_length)\n",
    "\n",
    "        # Stack all features for the dataset\n",
    "        features = np.vstack((mfcc, chroma, contrast, tonnetz))\n",
    "        features_dict[f\"{ms}ms\"] = features.T  # Transpose to have [timesteps, features]\n",
    "\n",
    "    return features_dict\n",
    "\n",
    "\n",
    "def extract_audio_features(file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    features_dict = extract_features_for_different_timesteps(y, sr)\n",
    "\n",
    "    # Compute the mean for a general representation (for CSV), using the default ~23ms hop_length\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "    features_mean = np.hstack(\n",
    "        (\n",
    "            np.mean(mfcc, axis=1),\n",
    "            np.mean(chroma, axis=1),\n",
    "            np.mean(contrast, axis=1),\n",
    "            np.mean(tonnetz, axis=1),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return features_mean, features_dict\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../train_test_validate_split.csv\")\n",
    "os.makedirs(\"output_h5\", exist_ok=True)\n",
    "\n",
    "# Prepare an empty list to hold row data\n",
    "rows_list = []\n",
    "h5_path = os.path.join(\"output_h5\", \"all_data.h5\")\n",
    "\n",
    "with h5py.File(h5_path, \"w\") as h5f:  # Open the file once\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            video_path = os.path.join(\n",
    "                \"../CDS datasets\",\n",
    "                \"CMU-MOSEI\",\n",
    "                \"Raw\",\n",
    "                str(row[\"video_id\"]),\n",
    "                f\"{row['clip_id']}.mp4\",\n",
    "            )\n",
    "\n",
    "            # Extract audio and save temporarily\n",
    "            audio_path = f\"temp_audio_{index}.wav\"\n",
    "            video = VideoFileClip(video_path)\n",
    "            video.audio.write_audiofile(audio_path)\n",
    "\n",
    "            # Extract features\n",
    "            features_mean, features_dict = extract_audio_features(audio_path)\n",
    "\n",
    "            # Prepare the row for the new DataFrame\n",
    "            new_row = row.to_dict()\n",
    "            new_row.update({f\"mfcc_{i+1}\": features_mean[i] for i in range(20)})\n",
    "            new_row.update({f\"chroma_{i+1}\": features_mean[20 + i] for i in range(12)})\n",
    "            new_row.update({f\"contrast_{i+1}\": features_mean[32 + i] for i in range(7)})\n",
    "            new_row.update({f\"tonnetz_{i+1}\": features_mean[39 + i] for i in range(6)})\n",
    "            rows_list.append(new_row)\n",
    "\n",
    "            # Group creation in the .h5 file\n",
    "            group_name = f\"{row['video_id']}_{row['clip_id']}\"\n",
    "            if group_name in h5f:\n",
    "                del h5f[group_name]  # Ensure no partial group from previous errors\n",
    "            grp = h5f.create_group(group_name)\n",
    "            grp.attrs[\"label\"] = row[\"annotation\"]\n",
    "            grp.attrs[\"text\"] = row[\"text\"]\n",
    "\n",
    "            # Create datasets for different timesteps\n",
    "            for timestep, features in features_dict.items():\n",
    "                dataset_name = f\"audio_features_{timestep}\"\n",
    "                grp.create_dataset(dataset_name, data=features)\n",
    "\n",
    "            # Cleanup the temporary audio file\n",
    "            os.remove(audio_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['video_id']}_{row['clip_id']}: {e}\")\n",
    "            # Ensure the group is removed if partially created\n",
    "            if group_name in h5f:\n",
    "                del h5f[group_name]\n",
    "            # Optionally, log or handle the error in other ways\n",
    "            continue  # Skip to the next datapoint\n",
    "\n",
    "\n",
    "columns_order = [col for col in new_row.keys() if col not in [\"annotation\"]] + [\n",
    "    \"annotation\"\n",
    "]\n",
    "new_df = pd.DataFrame(rows_list, columns=columns_order)\n",
    "\n",
    "# Save to new CSV\n",
    "new_df.to_csv(\"audio_features.csv\", index=False)\n",
    "## Normalize\n",
    "h5_path = \"output_h5/all_data.h5\"\n",
    "\n",
    "timesteps = [\"23ms\", \"100ms\", \"500ms\", \"1000ms\"]\n",
    "global_stats = {timestep: {\"mean\": None, \"std\": None} for timestep in timesteps}\n",
    "\n",
    "# Initialize lists to collect features for each timestep\n",
    "all_features = {timestep: [] for timestep in timesteps}\n",
    "\n",
    "with h5py.File(h5_path, \"r\") as h5f:\n",
    "    for group_name in h5f:\n",
    "        group = h5f[group_name]\n",
    "        for timestep in timesteps:\n",
    "            # Dataset name follows the format 'audio_features_{timestep}'\n",
    "            dataset_name = f\"audio_features_{timestep}\"\n",
    "            features = group[dataset_name][:]\n",
    "            all_features[timestep].append(features)\n",
    "\n",
    "# Compute global statistics for each timestep\n",
    "for timestep in timesteps:\n",
    "    features_concat = np.concatenate(all_features[timestep], axis=0)\n",
    "    global_stats[timestep][\"mean\"] = np.mean(features_concat, axis=0)\n",
    "    global_stats[timestep][\"std\"] = np.std(features_concat, axis=0)\n",
    "new_h5_path = \"output_h5/all_data_normalized.h5\"\n",
    "\n",
    "\n",
    "def apply_global_normalization(features, mean, std):\n",
    "    # Avoid division by zero\n",
    "    std_replaced = np.where(std == 0, 1, std)\n",
    "    return (features - mean) / std_replaced\n",
    "\n",
    "\n",
    "# Open the original file to read data\n",
    "with h5py.File(h5_path, \"r\") as h5f_original:\n",
    "    # Open the new file to write normalized data\n",
    "    with h5py.File(new_h5_path, \"w\") as h5f_normalized:\n",
    "        for group_name in h5f_original:\n",
    "            group_original = h5f_original[group_name]\n",
    "            group_normalized = h5f_normalized.create_group(\n",
    "                group_name\n",
    "            )  # Create corresponding group in new file\n",
    "\n",
    "            for timestep in timesteps:\n",
    "                dataset_name = f\"audio_features_{timestep}\"\n",
    "                original_features = group_original[dataset_name][:]\n",
    "                mean = global_stats[timestep][\"mean\"]\n",
    "                std = global_stats[timestep][\"std\"]\n",
    "                normalized_features = apply_global_normalization(\n",
    "                    original_features, mean, std\n",
    "                )\n",
    "\n",
    "                # Write normalized data to the new file\n",
    "                normalized_dataset_name = f\"{dataset_name}_normalized\"\n",
    "                group_normalized.create_dataset(\n",
    "                    normalized_dataset_name, data=normalized_features\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average the audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged features have been successfully saved to 'normalised_averaged_audio_features.h5'.\n"
     ]
    }
   ],
   "source": [
    "# Open the original h5 file\n",
    "h5_path = r'output_h5/normalised_audio_features.h5'\n",
    "with h5py.File(h5_path, 'r') as original_h5:\n",
    "    # Create a new h5 file to store the averaged features\n",
    "    with h5py.File(r'output_h5/normalised_averaged_audio_features.h5', 'w') as averaged_h5:\n",
    "        # Iterate through each group in the original h5 file\n",
    "        for group_name in original_h5:\n",
    "            group = original_h5[group_name]\n",
    "            \n",
    "            # Access the dataset containing the 23ms normalized audio features\n",
    "            dataset = group['audio_features_23ms_normalized']\n",
    "            \n",
    "            # Calculate the mean across all timesteps (axis 0)\n",
    "            averaged_data = np.mean(dataset, axis=0)\n",
    "            \n",
    "            # Create a new group in the new h5 file with the same name\n",
    "            new_group = averaged_h5.create_group(group_name)\n",
    "            \n",
    "            # Copy attributes from the original group to the new group\n",
    "            for attr_name, attr_value in group.attrs.items():\n",
    "                new_group.attrs[attr_name] = attr_value\n",
    "            \n",
    "            # Create a new dataset in the new group for the averaged data\n",
    "            new_group.create_dataset('audio_features_averaged', data=averaged_data)\n",
    "\n",
    "print(\"Averaged features have been successfully saved to 'normalised_averaged_audio_features.h5'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUTD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
