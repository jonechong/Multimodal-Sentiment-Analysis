{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import librosa\n",
    "from moviepy.editor import *\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_audio(path):\n",
    "    def calculate_hop_length(sr, ms):\n",
    "        return int(sr * ms / 1000)\n",
    "\n",
    "    def extract_features_for_different_timesteps(y, sr):\n",
    "        timesteps = [23, 100, 500, 1000]  # in milliseconds\n",
    "        features_dict = {}\n",
    "\n",
    "        for ms in timesteps:\n",
    "            hop_length = calculate_hop_length(sr, ms)\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=hop_length)\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\n",
    "            contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=hop_length)\n",
    "            tonnetz = librosa.feature.tonnetz(y=y, sr=sr, hop_length=hop_length)\n",
    "\n",
    "            # Stack all features for the dataset\n",
    "            features = np.vstack((mfcc, chroma, contrast, tonnetz))\n",
    "            features_dict[f\"{ms}ms\"] = features.T  # Transpose to have [timesteps, features]\n",
    "\n",
    "        return features_dict\n",
    "\n",
    "\n",
    "\n",
    "    def extract_audio_features(file_path):\n",
    "        y, sr = librosa.load(file_path)\n",
    "        features_dict = extract_features_for_different_timesteps(y, sr)\n",
    "\n",
    "        # Compute the mean for a general representation (for CSV), using the default ~23ms hop_length\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "        features_mean = np.hstack(\n",
    "            (\n",
    "                np.mean(mfcc, axis=1),\n",
    "                np.mean(chroma, axis=1),\n",
    "                np.mean(contrast, axis=1),\n",
    "                np.mean(tonnetz, axis=1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return features_mean, features_dict\n",
    "    # Extract audio and save temporarily\n",
    "    audio_path = f\"../temp\"\n",
    "    video = VideoFileClip(path)\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "\n",
    "    # Extract features\n",
    "    features_mean, features_dict = extract_audio_features(audio_path)\n",
    "    return features_mean, features_dict\n",
    "\n",
    "def proc_face(path):\n",
    "    # Load the pre-trained ResNet50 model\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "    # Modify pooling layer to global average pooling to get fixed size output. This results in a output vector of size 2048\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "    # Load OpenCV's Haar Cascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    def extract_facial_features(frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        if len(faces) == 0:\n",
    "            return None  # Return None if no faces are detected\n",
    "\n",
    "        (x, y, w, h) = faces[0]  # Consider the first face\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, (224, 224))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "        face = preprocess_input(face)\n",
    "        features = model.predict(face)\n",
    "        return features.flatten()\n",
    "    \n",
    "    frame_skip = 30\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    features_per_frame = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if cap.get(cv2.CAP_PROP_POS_FRAMES) % frame_skip == 0:\n",
    "            features = extract_facial_features(frame)\n",
    "            if features is not None:\n",
    "                features_per_frame.append(features)\n",
    "    cap.release()\n",
    "    if features_per_frame:\n",
    "            # Calculate the average of the features\n",
    "            average_features = np.mean(features_per_frame, axis=0)\n",
    "            return average_features\n",
    "\n",
    "def proc_text(path):\n",
    "    # Initialize the BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Function to encode text to BERT features with variable max_length\n",
    "    def encode_text_for_bert(text, max_length):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        feature_vector = torch.mean(embeddings, dim=1)\n",
    "        return feature_vector.squeeze().cpu().numpy()\n",
    "    bert_features = encode_text_for_bert(row['text'], 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_video(path):\n",
    "    audioproc = proc_audio(path)\n",
    "    faceproc = proc_face(path)\n",
    "    textproc = proc_text(path)\n",
    "    return \n",
    "def predict_sentiment(video_file):\n",
    "    # Preprocess the video file\n",
    "    video_data = preprocess_video(video_file)\n",
    "    \n",
    "    # Convert video data to PyTorch tensor\n",
    "    video_tensor = torch.tensor(video_data)  # Modify as per your data format\n",
    "    \n",
    "    # If GPU available, move tensor to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    video_tensor = video_tensor.to(device)\n",
    "    \n",
    "    # Load the model\n",
    "    model = YourModelClass()\n",
    "    model.load_state_dict(torch.load(\"sentiment_model.pth\"))\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(video_tensor)\n",
    "        \n",
    "        # Post-process outputs if necessary\n",
    "        # For example, apply softmax if the model outputs logits\n",
    "        \n",
    "        # Get predicted sentiment\n",
    "        predicted_sentiment = outputs.argmax().item()  # Modify based on your output format\n",
    "    \n",
    "    return predicted_sentiment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
