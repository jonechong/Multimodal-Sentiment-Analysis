{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "import librosa\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file in '../proc_csv/raw_videos'\n",
    "\n",
    "def extract_audio_features(audio_path, index,savePath):\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "        # Extract audio features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr).mean(axis=1)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr).mean(axis=1)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr).mean(axis=1)\n",
    "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr).mean(axis=1)\n",
    "\n",
    "        # Create a dictionary to hold features\n",
    "        features = {\n",
    "            'mfccs': mfccs.tolist(),  # Convert numpy arrays to lists\n",
    "            'chroma': chroma.tolist(),\n",
    "            'spectral_contrast': spectral_contrast.tolist(),\n",
    "            'tonnetz': tonnetz.tolist()\n",
    "        }\n",
    "\n",
    "        # Save features to a file, naming it with the DataFrame index for alignment\n",
    "        feature_filename = f'{savePath}/features_{index}.json'\n",
    "        with open(feature_filename, 'w') as file:\n",
    "            json.dump(features, file)\n",
    "\n",
    "        return index, feature_filename  # Return the index and filename for tracking\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {audio_path}: {e}\")\n",
    "        return index, None\n",
    "\n",
    "def process_video_batch(start_index, end_index, df, save_path, raw_video_path):\n",
    "    futures = []\n",
    "    video_lengths = {}  # Dictionary to store video lengths with DataFrame indices as keys\n",
    "\n",
    "    # Ensure the temp_audio directory exists\n",
    "    temp_audio_dir = os.path.join(save_path, 'temp_audio')\n",
    "    os.makedirs(temp_audio_dir, exist_ok=True)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        for index, row in df.iloc[start_index:end_index].iterrows():\n",
    "            video_path = os.path.join(raw_video_path, f\"{row['video_id']}/{row['clip_id']}.mp4\")\n",
    "            temp_audio_path = os.path.join(temp_audio_dir, f\"{row['video_id']}_{row['clip_id']}.wav\")\n",
    "\n",
    "            try:\n",
    "                video_clip = VideoFileClip(video_path)\n",
    "                video_clip.audio.write_audiofile(temp_audio_path)\n",
    "                video_clip.close()\n",
    "\n",
    "                # Submit audio feature extraction task along with the index and path to save the features\n",
    "                futures.append(executor.submit(extract_audio_features, temp_audio_path, index, save_path))\n",
    "\n",
    "                # Store video length in the dictionary using the original DataFrame index\n",
    "                video_lengths[index] = video_clip.duration\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting audio from {video_path}: {e}\")\n",
    "\n",
    "            finally:\n",
    "                if os.path.exists(temp_audio_path):\n",
    "                    os.remove(temp_audio_path)\n",
    "\n",
    "    # Process future results for audio features\n",
    "    for future in futures:\n",
    "        index, feature_filename = future.result()\n",
    "        if feature_filename:\n",
    "            print(f\"Features extracted and saved for index {index}: {feature_filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to process video at index {index}\")\n",
    "\n",
    "    return video_lengths\n",
    "\n",
    "def load_features_from_json(file_path):\n",
    "    # Extract the index from the file name using a regular expression\n",
    "    # Assuming the file name format is \"features_{index}.json\"\n",
    "    match = re.search(r'features_(\\d+).json', file_path)\n",
    "    if match:\n",
    "        index = int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"Index not found in file name: {file_path}\")\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        features = json.load(file)\n",
    "\n",
    "    return index, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_proc():\n",
    "# Adjust the path below to the location of your CSV file within Google Drive\n",
    "    csv_file_path = './label.csv'\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Filter the necessary columns\n",
    "    filtered_df = df[['video_id', 'clip_id', 'text', 'annotation']]\n",
    "\n",
    "    batch_size = 100  # Define your batch size\n",
    "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Create the directory for storing video lengths if it doesn't exist\n",
    "    video_lengths_dir = '../proc_csv/videoLengths'\n",
    "    os.makedirs(video_lengths_dir, exist_ok=True)\n",
    "\n",
    "    successful_indices = set()  # To keep track of successful indices\n",
    "\n",
    "    for batch_num in range(total_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, len(df))\n",
    "        print(f\"Processing batch {batch_num + 1}/{total_batches}: Indices {start_index} to {end_index}\")\n",
    "\n",
    "        batch_video_lengths = process_video_batch(\n",
    "            start_index, end_index, df,\n",
    "            '../proc_csv/temp_audio',\n",
    "            '../proc_csv/raw_videos'\n",
    "        )\n",
    "\n",
    "        # Update successful indices based on this batch's results\n",
    "        successful_indices.update(batch_video_lengths.keys())\n",
    "\n",
    "        # Save this batch's video lengths to a file\n",
    "        batch_file_path = os.path.join(video_lengths_dir, f'video_lengths_batch_{batch_num + 1}.json')\n",
    "        with open(batch_file_path, 'w') as file:\n",
    "            json.dump(batch_video_lengths, file)\n",
    "\n",
    "        print(f\"Saved video lengths for batch {batch_num + 1} to {batch_file_path}\")\n",
    "\n",
    "    # Initialize an empty dictionary to compile all video lengths\n",
    "    compiled_video_lengths = {}\n",
    "\n",
    "    # Read each batch's video lengths from their respective files and compile them\n",
    "    for batch_num in range(total_batches):\n",
    "        batch_file_path = os.path.join(video_lengths_dir, f'video_lengths_batch_{batch_num + 1}.json')\n",
    "        with open(batch_file_path, 'r') as file:\n",
    "            batch_video_lengths = json.load(file)\n",
    "            compiled_video_lengths.update(batch_video_lengths)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Path where your JSON files are stored\n",
    "    json_files_path = '../proc_csv/*.json'\n",
    "    json_files = glob.glob(json_files_path)\n",
    "\n",
    "    all_features = []\n",
    "\n",
    "    for file_path in json_files:\n",
    "        index, features = load_features_from_json(file_path)\n",
    "        all_features.append((index, features))\n",
    "\n",
    "    # Initialize a list to hold the feature vectors\n",
    "    feature_vectors = []\n",
    "\n",
    "    # Initialize a list to hold the indices\n",
    "    indices = []\n",
    "\n",
    "    # Number of features for each type\n",
    "    n_mfcc, n_chroma, n_contrast, n_tonnetz = 20, 12, 7, 6\n",
    "\n",
    "    # Define column labels\n",
    "    mfcc_labels = [f'mfcc_{i}' for i in range(1, n_mfcc + 1)]\n",
    "    chroma_labels = [f'chroma_{i}' for i in range(1, n_chroma + 1)]\n",
    "    contrast_labels = [f'contrast_{i}' for i in range(1, n_contrast + 1)]\n",
    "    tonnetz_labels = [f'tonnetz_{i}' for i in range(1, n_tonnetz + 1)]\n",
    "\n",
    "    column_labels = mfcc_labels + chroma_labels + contrast_labels + tonnetz_labels\n",
    "\n",
    "    for index, features in all_features:\n",
    "        # Flatten the features into a single vector and append to the feature_vectors list\n",
    "        feature_vector = features['mfccs'] + features['chroma'] + features['spectral_contrast'] + features['tonnetz']\n",
    "        feature_vectors.append(feature_vector)\n",
    "\n",
    "        # Append the index to the indices list\n",
    "        indices.append(index)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df_features = pd.DataFrame(feature_vectors, columns=column_labels)\n",
    "\n",
    "    # Set the DataFrame's index to the extracted indices\n",
    "    df_features.index = indices\n",
    "\n",
    "    # Sort the DataFrame by index to ensure proper alignment\n",
    "    df_features.sort_index(inplace=True)\n",
    "\n",
    "    # Convert the compiled video lengths to a DataFrame\n",
    "    video_lengths_df = pd.DataFrame(list(compiled_video_lengths.items()), columns=['index', 'video_length'])\n",
    "    video_lengths_df.set_index('index', inplace=True)\n",
    "    if video_lengths_df.index.dtype == 'object':\n",
    "        video_lengths_df.index = video_lengths_df.index.astype('int')\n",
    "    merged_df = df_features.merge(video_lengths_df, left_index=True, right_index=True, how='inner')\n",
    "    merged_df.drop('index', axis=1, inplace=True)\n",
    "    merged_df.head()\n",
    "\n",
    "    # Define the file path where you want to save the DataFrame\n",
    "    file_path = '../proc_csv/AudioFeaturesMOSI.csv'\n",
    "\n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    merged_df.to_csv(file_path, index=False)\n",
    "\n",
    "    # Assuming 'video_length' column exists in the original merged_df\n",
    "    average_length = merged_df['video_length'].mean()\n",
    "\n",
    "    print(f\"Average video length: {average_length:.2f} seconds\")\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scalerMinMax = MinMaxScaler()\n",
    "    # Reshape the 'video_length' column to a 2D array\n",
    "    video_lengths_reshaped = merged_df['video_length'].values.reshape(-1, 1)\n",
    "\n",
    "    # Fit the scaler to the data and transform it\n",
    "    scaled_video_lengths = scalerMinMax.fit_transform(video_lengths_reshaped)\n",
    "\n",
    "    # Select only the feature columns (excluding non-feature columns if any, such as text labels or video IDs)\n",
    "    feature_columns = [col for col in merged_df.columns if col not in ['video_length']]  # Adjust non-feature column names as needed\n",
    "    features_to_normalize = merged_df[feature_columns]\n",
    "\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler to the features and transform them\n",
    "    normalized_features = scaler.fit_transform(features_to_normalize)\n",
    "\n",
    "    # Create a new DataFrame with the normalized features\n",
    "    normalized_df = pd.DataFrame(normalized_features, columns=feature_columns, index=merged_df.index)\n",
    "    normalized_df['video_length'] = scaled_video_lengths\n",
    "\n",
    "    class_labels_for_normalized_df = filtered_df.loc[normalized_df.index, 'annotation']\n",
    "    normalized_df['class_labels'] = class_labels_for_normalized_df\n",
    "\n",
    "    # Define the file path where you want to save the DataFrame\n",
    "    file_path = '../proc_csv/AudioFeaturesMOSINormalised.csv'\n",
    "\n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    normalized_df.to_csv(file_path, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
