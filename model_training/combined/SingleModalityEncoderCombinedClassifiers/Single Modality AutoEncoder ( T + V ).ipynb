{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661414f9-aaad-427d-8643-a9286d45f5e5",
   "metadata": {},
   "source": [
    "# Single Modality AutoEncoder ( T + V )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8402c-5021-4675-ab27-bc0d1c1501a9",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a772c-0efa-49dc-ba66-0db3a47f70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3028fcb-3dc9-436c-b538-01164fedbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8a1b4-0e4e-405e-922d-9c292f3bff9f",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd91d9d2-34ec-4378-8f13-7b0f34dbcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, file_path, bert_feature_size='bert_text_features_128', split='train', dtype=torch.float32):\n",
    "        self.file_path = file_path\n",
    "        self.bert_feature_size = bert_feature_size\n",
    "        self.split = split\n",
    "        self.dtype = dtype  \n",
    "        self.data_keys = []\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            for key in file.keys():\n",
    "                if file[key].attrs['split'] == self.split:\n",
    "                    self.data_keys.append(key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "\n",
    "            group_key = self.data_keys[idx]\n",
    "            group = file[group_key]\n",
    "\n",
    "            label = group.attrs['label']\n",
    "            text = group.attrs['text']\n",
    "            audio_features = torch.from_numpy(group['audio_features_averaged'][()]).type(self.dtype)\n",
    "            facial_features = torch.from_numpy(group['averaged_facial_features'][()]).type(self.dtype)\n",
    "            bert_features = torch.from_numpy(group[self.bert_feature_size][()]).type(self.dtype)\n",
    "\n",
    "            # Convert label to a numeric format, if necessary\n",
    "            label_to_index = {'Positive': 2, 'Neutral': 1, 'Negative': 0}\n",
    "            label_index = label_to_index[label]\n",
    "\n",
    "            # Create a dictionary with the data\n",
    "            sample = {\n",
    "                'label': label_index,\n",
    "                'text': text,\n",
    "                'audio_features': audio_features,\n",
    "                'facial_features': facial_features,\n",
    "                'bert_features': bert_features\n",
    "            }\n",
    "\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b2ebea-3ab3-44e6-88e2-a8766cf079ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './combined_features.h5'\n",
    "\n",
    "datasets = {}\n",
    "bert_feature_sizes = ['bert_text_features_128', 'bert_text_features_256', 'bert_text_features_512']\n",
    "splits = ['train', 'validate', 'test']\n",
    "\n",
    "for feature_size in bert_feature_sizes:\n",
    "    datasets[feature_size] = {}\n",
    "    for split in splits:\n",
    "        dataset_key = f\"{split}_{feature_size}\"\n",
    "        datasets[feature_size][split] = CombinedDataset(file_path, bert_feature_size=feature_size, split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5405aa-4908-4e3c-9f4a-bfe915f7e58c",
   "metadata": {},
   "source": [
    "### Common Trainer Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8732a673-96f9-45a1-a2c5-35f08e17bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_dataset, val_dataset, model_name, epochs, save_interval, lr=1e-3, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.model_name = model_name\n",
    "        self.start_epoch = 0 \n",
    "        self.epochs = epochs\n",
    "        self.save_interval = save_interval\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.history = defaultdict(list)\n",
    "        self.checkpoint_dir = f'modelCheckPoints/{self.model_name}'\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        self.optimizer = None \n",
    "\n",
    "    def initialize_optimizer(self):\n",
    "        sample_batch = next(iter(DataLoader(self.train_dataset, batch_size=1, shuffle=True)))\n",
    "        \n",
    "        bert_features = sample_batch['bert_features'].to(self.device)\n",
    "        facial_features = sample_batch['facial_features'].to(self.device)\n",
    "  \n",
    "        _ = self.model(bert_features,facial_features)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        \n",
    "    def save_checkpoint(self, epoch):\n",
    "        state = {'epoch': epoch, 'state_dict': self.model.state_dict()}\n",
    "        torch.save(state, f'{self.checkpoint_dir}/{epoch}.pt')\n",
    "\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        checkpoints = [ckpt for ckpt in os.listdir(self.checkpoint_dir) if ckpt.endswith('.pt')]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('.')[0]))\n",
    "            checkpoint = torch.load(f'{self.checkpoint_dir}/{latest_checkpoint}', map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.start_epoch = checkpoint['epoch'] + 1  \n",
    "            print(f\"Loaded checkpoint: {latest_checkpoint} at epoch {checkpoint['epoch']}\")\n",
    "        else:\n",
    "            self.start_epoch = 0  \n",
    "            print(\"No checkpoints found, starting from scratch.\")\n",
    "\n",
    "    def save_history(self):\n",
    "        with open(f'{self.checkpoint_dir}/history.json', 'w') as f:\n",
    "            json.dump(self.history, f)\n",
    "            \n",
    "    def train_one_epoch(self, dataloader, criterion):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "    \n",
    "        for batch in dataloader:\n",
    "            bert_features = batch['bert_features'].to(self.device)\n",
    "            facial_features = batch['facial_features'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(bert_features, facial_features)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        accuracy = correct_predictions / len(dataloader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "\n",
    "    \n",
    "    def validate(self, dataloader, criterion):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                bert_features = batch['bert_features'].to(self.device)\n",
    "                facial_features = batch['facial_features'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                outputs = self.model(bert_features, facial_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        accuracy = correct_predictions / len(dataloader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, criterion):\n",
    "        self.initialize_optimizer()\n",
    "        self.load_checkpoint()\n",
    "    \n",
    "        train_dataloader = DataLoader(self.train_dataset, batch_size=32, shuffle=True)\n",
    "        val_dataloader = DataLoader(self.val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "        try:\n",
    "            for epoch in range(self.start_epoch, self.epochs):\n",
    "                train_loss, train_acc = self.train_one_epoch(train_dataloader, criterion)\n",
    "                val_loss, val_acc = self.validate(val_dataloader, criterion)\n",
    "    \n",
    "                self.history['train_loss'].append(train_loss)\n",
    "                self.history['train_acc'].append(train_acc)\n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_acc'].append(val_acc)\n",
    "    \n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, \"\n",
    "                      f\"Train Accuracy: {train_acc:.4f}, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, \"\n",
    "                      f\"Val Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "                if (epoch + 1) % self.save_interval == 0:\n",
    "                    self.save_checkpoint(epoch + 1)\n",
    "    \n",
    "                self.save_history()\n",
    "    \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user. Saving last model state...\")\n",
    "            self.save_checkpoint(epoch + 1)\n",
    "            self.save_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19cdb8d-88d3-4d9b-ad72-4715259b169a",
   "metadata": {},
   "source": [
    "### Text AutoEncoder Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405fa02c-c5c9-4780-b8af-b8b42dd56c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextAutoEncoder, self).__init__()\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.size(1)\n",
    "        if self.encoder is None or self.decoder is None:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 64)\n",
    "            ).to(x.device) \n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(64, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, input_size),\n",
    "                nn.Sigmoid()\n",
    "            ).to(x.device) \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89140ecd-08c9-492c-9f42-737349a04e26",
   "metadata": {},
   "source": [
    "### Video AutoEncoder Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8c3ce8f7-dfdf-48e8-b9dc-f7ca32f10335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoAutoEncoder, self).__init__()\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.size(1) \n",
    "        if self.encoder is None or self.decoder is None:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_size, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 128)\n",
    "            ).to(x.device)\n",
    "\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(128, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, input_size),\n",
    "                nn.Sigmoid()\n",
    "            ).to(x.device)\n",
    "\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4179c552-859b-4670-b11a-68da8408d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CombinedClassifier, self).__init__()\n",
    "        self.text_autoencoder = TextAutoEncoder()\n",
    "        self.video_autoencoder = VideoAutoEncoder()\n",
    "        self.classifier = None\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self,bert_features,facial_features):        \n",
    "        text_encoded, _ = self.text_autoencoder(bert_features)\n",
    "        video_encoded, _ = self.video_autoencoder(facial_features)\n",
    "        \n",
    "        combined_feature_size = text_encoded.size(1) + video_encoded.size(1)\n",
    "        if self.classifier is None:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(combined_feature_size, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            ).to(bert_features.device) \n",
    "        combined_features = torch.cat((text_encoded, video_encoded), dim=1)\n",
    "    \n",
    "        class_logits = self.classifier(combined_features)\n",
    "        return class_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b64e63-967f-40ec-84db-717c98aed8f2",
   "metadata": {},
   "source": [
    "### Training with bert 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "76a66b75-d0f0-4335-aec8-0e7665a6745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, starting from scratch.\n",
      "Epoch 1/25, Train Loss: 0.0270, Train Accuracy: 0.5960, Val Loss: 0.0249, Val Accuracy: 0.6415\n",
      "Epoch 2/25, Train Loss: 0.0246, Train Accuracy: 0.6422, Val Loss: 0.0245, Val Accuracy: 0.6450\n",
      "Epoch 3/25, Train Loss: 0.0238, Train Accuracy: 0.6608, Val Loss: 0.0246, Val Accuracy: 0.6372\n",
      "Epoch 4/25, Train Loss: 0.0231, Train Accuracy: 0.6700, Val Loss: 0.0251, Val Accuracy: 0.6315\n",
      "Epoch 5/25, Train Loss: 0.0223, Train Accuracy: 0.6874, Val Loss: 0.0247, Val Accuracy: 0.6336\n",
      "Epoch 6/25, Train Loss: 0.0215, Train Accuracy: 0.6995, Val Loss: 0.0252, Val Accuracy: 0.6408\n",
      "Epoch 7/25, Train Loss: 0.0206, Train Accuracy: 0.7137, Val Loss: 0.0257, Val Accuracy: 0.6322\n",
      "Epoch 8/25, Train Loss: 0.0197, Train Accuracy: 0.7319, Val Loss: 0.0260, Val Accuracy: 0.6222\n",
      "Epoch 9/25, Train Loss: 0.0185, Train Accuracy: 0.7461, Val Loss: 0.0276, Val Accuracy: 0.6208\n",
      "Epoch 10/25, Train Loss: 0.0176, Train Accuracy: 0.7615, Val Loss: 0.0280, Val Accuracy: 0.6101\n",
      "Epoch 11/25, Train Loss: 0.0161, Train Accuracy: 0.7840, Val Loss: 0.0306, Val Accuracy: 0.6087\n",
      "Epoch 12/25, Train Loss: 0.0151, Train Accuracy: 0.7974, Val Loss: 0.0331, Val Accuracy: 0.6044\n",
      "Epoch 13/25, Train Loss: 0.0137, Train Accuracy: 0.8196, Val Loss: 0.0344, Val Accuracy: 0.6130\n",
      "Epoch 14/25, Train Loss: 0.0126, Train Accuracy: 0.8351, Val Loss: 0.0356, Val Accuracy: 0.5994\n",
      "Epoch 15/25, Train Loss: 0.0115, Train Accuracy: 0.8503, Val Loss: 0.0394, Val Accuracy: 0.6087\n",
      "Epoch 16/25, Train Loss: 0.0107, Train Accuracy: 0.8638, Val Loss: 0.0422, Val Accuracy: 0.6130\n",
      "Epoch 17/25, Train Loss: 0.0095, Train Accuracy: 0.8772, Val Loss: 0.0478, Val Accuracy: 0.6058\n",
      "Epoch 18/25, Train Loss: 0.0087, Train Accuracy: 0.8856, Val Loss: 0.0522, Val Accuracy: 0.6030\n",
      "Epoch 19/25, Train Loss: 0.0085, Train Accuracy: 0.8936, Val Loss: 0.0494, Val Accuracy: 0.6080\n",
      "Epoch 20/25, Train Loss: 0.0079, Train Accuracy: 0.8973, Val Loss: 0.0539, Val Accuracy: 0.6051\n",
      "Epoch 21/25, Train Loss: 0.0071, Train Accuracy: 0.9107, Val Loss: 0.0537, Val Accuracy: 0.5980\n",
      "Epoch 22/25, Train Loss: 0.0063, Train Accuracy: 0.9214, Val Loss: 0.0569, Val Accuracy: 0.5959\n",
      "Epoch 23/25, Train Loss: 0.0055, Train Accuracy: 0.9314, Val Loss: 0.0621, Val Accuracy: 0.5852\n",
      "Epoch 24/25, Train Loss: 0.0051, Train Accuracy: 0.9356, Val Loss: 0.0790, Val Accuracy: 0.5709\n",
      "Epoch 25/25, Train Loss: 0.0051, Train Accuracy: 0.9372, Val Loss: 0.0674, Val Accuracy: 0.6037\n"
     ]
    }
   ],
   "source": [
    "train_dataset_512 = datasets['bert_text_features_512']['train']\n",
    "validate_dataset_512 = datasets['bert_text_features_512']['validate']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = 3\n",
    "combinedClassifier_BERT512_TE1_VE1 = CombinedClassifier(num_classes=num_classes).to(device)\n",
    "modelName = 'CombinedClassifier_BERT512_TE1_VE1'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainer = ModelTrainer(combinedClassifier_BERT512_TE1_VE1, train_dataset_512, validate_dataset_512, modelName, epochs=25, save_interval=5)\n",
    "trainer.train(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d81563-5f92-4748-a2f4-26f58115fe8b",
   "metadata": {},
   "source": [
    "# Model 2 (More robust classifier, but including regularsiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f2c320e5-4d28-49df-99a4-29f3179191cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedClassifier2(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super(CombinedClassifier2, self).__init__()\n",
    "        self.text_autoencoder = TextAutoEncoder()\n",
    "        self.video_autoencoder = VideoAutoEncoder()\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward(self, bert_features, facial_features):        \n",
    "        text_encoded, _ = self.text_autoencoder(bert_features)\n",
    "        video_encoded, _ = self.video_autoencoder(facial_features)\n",
    "        \n",
    "        combined_feature_size = text_encoded.size(1) + video_encoded.size(1)\n",
    "        if self.classifier is None:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(combined_feature_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            ).to(bert_features.device) \n",
    "        combined_features = torch.cat((text_encoded, video_encoded), dim=1)\n",
    "        \n",
    "        class_logits = self.classifier(combined_features)\n",
    "        return class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ba6b1e49-8847-486f-ad4a-1f1cddd67542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, starting from scratch.\n",
      "Epoch 1/25, Train Loss: 0.0298, Train Accuracy: 0.5233, Val Loss: 0.0253, Val Accuracy: 0.6279\n",
      "Epoch 2/25, Train Loss: 0.0264, Train Accuracy: 0.6225, Val Loss: 0.0249, Val Accuracy: 0.6237\n",
      "Epoch 3/25, Train Loss: 0.0256, Train Accuracy: 0.6380, Val Loss: 0.0246, Val Accuracy: 0.6458\n",
      "Epoch 4/25, Train Loss: 0.0251, Train Accuracy: 0.6382, Val Loss: 0.0247, Val Accuracy: 0.6365\n",
      "Epoch 5/25, Train Loss: 0.0244, Train Accuracy: 0.6537, Val Loss: 0.0257, Val Accuracy: 0.6101\n",
      "Epoch 6/25, Train Loss: 0.0238, Train Accuracy: 0.6638, Val Loss: 0.0249, Val Accuracy: 0.6208\n",
      "Epoch 7/25, Train Loss: 0.0235, Train Accuracy: 0.6717, Val Loss: 0.0250, Val Accuracy: 0.6329\n",
      "Epoch 8/25, Train Loss: 0.0227, Train Accuracy: 0.6786, Val Loss: 0.0246, Val Accuracy: 0.6265\n",
      "Epoch 9/25, Train Loss: 0.0223, Train Accuracy: 0.6897, Val Loss: 0.0259, Val Accuracy: 0.6165\n",
      "Epoch 10/25, Train Loss: 0.0217, Train Accuracy: 0.7035, Val Loss: 0.0254, Val Accuracy: 0.6087\n",
      "\n",
      "Training interrupted by user. Saving last model state...\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3  \n",
    "combinedClassifier2_BERT512_TE1_VE1 = CombinedClassifier2(num_classes=num_classes,dropout_rate=0.8).to(device)\n",
    "modelName = 'combinedClassifier2_BERT512_TE1_VE1IncreasedRegularisation'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainer2 = ModelTrainer(combinedClassifier2_BERT512_TE1_VE1, train_dataset_512, validate_dataset_512, modelName, epochs=25, save_interval=5)\n",
    "trainer2.train(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5928bbb9-d4ae-40bf-b26a-8495986e6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 (modify the videoAutoEncoder to add a hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff8aef7-56a2-4659-8f91-8d10e1dcc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAutoEncoder2(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(VideoAutoEncoder2, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.size(1)  # Dynamically determine the input size\n",
    "\n",
    "        if self.encoder is None or self.decoder is None:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(256, 128)\n",
    "            ).to(x.device)\n",
    "\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(128, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(256, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(512, input_size),\n",
    "                nn.Sigmoid()\n",
    "            ).to(x.device)\n",
    "\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573f184a-bb08-4d66-b63d-454d082b2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedClassifier3(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super(CombinedClassifier3, self).__init__()\n",
    "        self.text_autoencoder = TextAutoEncoder()\n",
    "        self.video_autoencoder = VideoAutoEncoder2()\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward(self, bert_features, facial_features):        \n",
    "        text_encoded, _ = self.text_autoencoder(bert_features)\n",
    "        video_encoded, _ = self.video_autoencoder(facial_features)\n",
    "        \n",
    "        combined_feature_size = text_encoded.size(1) + video_encoded.size(1)\n",
    "        if self.classifier is None:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(combined_feature_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            ).to(bert_features.device)  \n",
    "\n",
    "        combined_features = torch.cat((text_encoded, video_encoded), dim=1)\n",
    "        \n",
    "        class_logits = self.classifier(combined_features)\n",
    "        return class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "029b4e5d-d2ef-4f40-86a5-559fa135de6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: 6.pt\n",
      "Epoch 1/25, Train Loss: 0.0224, Train Accuracy: 0.6879, Val Loss: 0.0250, Val Accuracy: 0.6436\n",
      "Epoch 2/25, Train Loss: 0.0214, Train Accuracy: 0.7033, Val Loss: 0.0257, Val Accuracy: 0.6180\n",
      "Epoch 3/25, Train Loss: 0.0208, Train Accuracy: 0.7110, Val Loss: 0.0261, Val Accuracy: 0.6230\n",
      "Epoch 4/25, Train Loss: 0.0197, Train Accuracy: 0.7297, Val Loss: 0.0277, Val Accuracy: 0.6172\n",
      "Epoch 5/25, Train Loss: 0.0191, Train Accuracy: 0.7389, Val Loss: 0.0270, Val Accuracy: 0.6215\n",
      "\n",
      "Training interrupted by user. Saving last model state...\n"
     ]
    }
   ],
   "source": [
    "train_dataset_512 = datasets['bert_text_features_512']['train']\n",
    "validate_dataset_512 = datasets['bert_text_features_512']['validate']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "num_classes = 3 \n",
    "combinedClassifier3_BERT512_TE1_VE2 = CombinedClassifier3(num_classes=num_classes,dropout_rate=0.5).to(device)\n",
    "modelName = 'combinedClassifier3_BERT512_TE1_VE2'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer3 = ModelTrainer(combinedClassifier3_BERT512_TE1_VE2, train_dataset_512, validate_dataset_512, modelName, epochs=25, save_interval=3)\n",
    "\n",
    "trainer3.train(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7a19e-8b37-4d52-8993-c04537e7cf13",
   "metadata": {},
   "source": [
    "# Model 4: Directly using Bert Features , plus a more stronger video Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce89dfe7-ea0f-41d3-92f3-eecc59f58eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAutoEncoder3(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(VideoAutoEncoder2, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.size(1) \n",
    "\n",
    "        if self.encoder is None or self.decoder is None:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_size, 256),  \n",
    "                nn.LeakyReLU(0.1),  \n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(256, 128), \n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(self.dropout_rate)\n",
    "            ).to(x.device)\n",
    "\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(128, 256),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(256, input_size),\n",
    "                nn.Sigmoid() \n",
    "            ).to(x.device)\n",
    "\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "639227ff-dd6e-4100-b28c-7808e8725d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedClassifier4(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super(CombinedClassifier4, self).__init__()\n",
    "        self.video_autoencoder = VideoAutoEncoder3(dropout_rate=dropout_rate)\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = None\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, bert_features, facial_features):        \n",
    "        video_encoded, _ = self.video_autoencoder(facial_features)\n",
    "        \n",
    "        combined_feature_size = bert_features.size(1) + video_encoded.size(1)\n",
    "        if self.classifier is None:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(combined_feature_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            ).to(bert_features.device)\n",
    "    \n",
    "        combined_features = torch.cat((bert_features, video_encoded), dim=1)\n",
    "\n",
    "        class_logits = self.classifier(combined_features)\n",
    "        return class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2749e1fd-0d1a-456d-9a3f-14b7501683f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, starting from scratch.\n",
      "Epoch 1/25, Train Loss: 0.0278, Train Accuracy: 0.5744, Val Loss: 0.0253, Val Accuracy: 0.6080\n",
      "Epoch 2/25, Train Loss: 0.0255, Train Accuracy: 0.6295, Val Loss: 0.0247, Val Accuracy: 0.6386\n",
      "Epoch 3/25, Train Loss: 0.0249, Train Accuracy: 0.6387, Val Loss: 0.0242, Val Accuracy: 0.6415\n",
      "Epoch 4/25, Train Loss: 0.0244, Train Accuracy: 0.6431, Val Loss: 0.0246, Val Accuracy: 0.6436\n",
      "Epoch 5/25, Train Loss: 0.0241, Train Accuracy: 0.6476, Val Loss: 0.0251, Val Accuracy: 0.6329\n",
      "Epoch 6/25, Train Loss: 0.0239, Train Accuracy: 0.6521, Val Loss: 0.0241, Val Accuracy: 0.6422\n",
      "Epoch 7/25, Train Loss: 0.0235, Train Accuracy: 0.6664, Val Loss: 0.0239, Val Accuracy: 0.6493\n",
      "Epoch 8/25, Train Loss: 0.0232, Train Accuracy: 0.6672, Val Loss: 0.0248, Val Accuracy: 0.6336\n",
      "Epoch 9/25, Train Loss: 0.0231, Train Accuracy: 0.6768, Val Loss: 0.0238, Val Accuracy: 0.6600\n",
      "Epoch 10/25, Train Loss: 0.0224, Train Accuracy: 0.6827, Val Loss: 0.0239, Val Accuracy: 0.6536\n",
      "Epoch 11/25, Train Loss: 0.0222, Train Accuracy: 0.6815, Val Loss: 0.0252, Val Accuracy: 0.6287\n",
      "Epoch 12/25, Train Loss: 0.0219, Train Accuracy: 0.6943, Val Loss: 0.0247, Val Accuracy: 0.6422\n",
      "Epoch 13/25, Train Loss: 0.0214, Train Accuracy: 0.7021, Val Loss: 0.0245, Val Accuracy: 0.6472\n",
      "Epoch 14/25, Train Loss: 0.0210, Train Accuracy: 0.7049, Val Loss: 0.0245, Val Accuracy: 0.6486\n",
      "Epoch 15/25, Train Loss: 0.0208, Train Accuracy: 0.7116, Val Loss: 0.0250, Val Accuracy: 0.6379\n",
      "\n",
      "Training interrupted by user. Saving last model state...\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3 \n",
    "cc4_TE0_VE3_r_04 = CombinedClassifier4(num_classes=num_classes,dropout_rate=0.4).to(device)\n",
    "modelName = 'cc4_TE0_VE3_r_04'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer5 = ModelTrainer(cc4_TE0_VE3_r_04, train_dataset_512, validate_dataset_512, modelName, epochs=25, save_interval=3)\n",
    "\n",
    "trainer5.train(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a798d6-8d12-49b4-a000-9f62b07107ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: Directly using Bert Features , plus a more stronger video Encoder (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00af928b-8105-4cb8-95c2-eab2f3b6f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAutoEncoder4(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(VideoAutoEncoder4, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Define the size of the intermediate layers\n",
    "        self.mid_size = 256\n",
    "        self.enc_out_size = 128\n",
    "        self.encoder_first = None\n",
    "        self.path1 = None\n",
    "        self.path2 = None\n",
    "        self.decoder = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.encoder_first is None:\n",
    "            input_size = x.size(1)\n",
    "            self.encoder_first = nn.Linear(input_size, self.mid_size).to(x.device)\n",
    "            self.path1 = nn.Sequential(\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(self.mid_size, self.mid_size) \n",
    "            ).to(x.device)\n",
    "            self.path2 = nn.Sequential(\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(self.mid_size, self.mid_size) \n",
    "            ).to(x.device)\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(self.mid_size, input_size), \n",
    "                nn.Sigmoid()\n",
    "            ).to(x.device)\n",
    "\n",
    "        x_enc = self.encoder_first(x)\n",
    "        path1_output = self.path1(x_enc)\n",
    "        path2_output = self.path2(x_enc)\n",
    "        encoded = path1_output + path2_output + x_enc\n",
    "\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8af9f4d-d59b-482f-9cd3-361a3addf3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedClassifier5(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super(CombinedClassifier5, self).__init__()\n",
    "        self.video_autoencoder = VideoAutoEncoder4(dropout_rate=dropout_rate)\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = None\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, bert_features, facial_features):        \n",
    "        video_encoded, _ = self.video_autoencoder(facial_features)\n",
    "        \n",
    "        combined_feature_size = bert_features.size(1) + video_encoded.size(1)\n",
    "        if self.classifier is None:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(combined_feature_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            ).to(bert_features.device)\n",
    "        \n",
    "        combined_features = torch.cat((bert_features, video_encoded), dim=1)\n",
    "\n",
    "        class_logits = self.classifier(combined_features)\n",
    "        return class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54a25a57-f00a-4272-9716-e7b434e4b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, starting from scratch.\n",
      "Epoch 1/25, Train Loss: 0.0295, Train Accuracy: 0.5359, Val Loss: 0.0271, Val Accuracy: 0.5766\n",
      "Epoch 2/25, Train Loss: 0.0269, Train Accuracy: 0.6000, Val Loss: 0.0263, Val Accuracy: 0.6023\n",
      "Epoch 3/25, Train Loss: 0.0260, Train Accuracy: 0.6176, Val Loss: 0.0267, Val Accuracy: 0.5895\n",
      "Epoch 4/25, Train Loss: 0.0256, Train Accuracy: 0.6268, Val Loss: 0.0245, Val Accuracy: 0.6329\n",
      "Epoch 5/25, Train Loss: 0.0252, Train Accuracy: 0.6286, Val Loss: 0.0247, Val Accuracy: 0.6486\n",
      "Epoch 6/25, Train Loss: 0.0249, Train Accuracy: 0.6404, Val Loss: 0.0254, Val Accuracy: 0.6101\n",
      "Epoch 7/25, Train Loss: 0.0244, Train Accuracy: 0.6451, Val Loss: 0.0246, Val Accuracy: 0.6386\n",
      "Epoch 8/25, Train Loss: 0.0242, Train Accuracy: 0.6505, Val Loss: 0.0249, Val Accuracy: 0.6336\n",
      "Epoch 9/25, Train Loss: 0.0240, Train Accuracy: 0.6548, Val Loss: 0.0257, Val Accuracy: 0.6208\n",
      "Epoch 10/25, Train Loss: 0.0241, Train Accuracy: 0.6500, Val Loss: 0.0252, Val Accuracy: 0.6379\n",
      "Epoch 11/25, Train Loss: 0.0236, Train Accuracy: 0.6624, Val Loss: 0.0248, Val Accuracy: 0.6201\n",
      "Epoch 12/25, Train Loss: 0.0233, Train Accuracy: 0.6638, Val Loss: 0.0243, Val Accuracy: 0.6344\n",
      "Epoch 13/25, Train Loss: 0.0230, Train Accuracy: 0.6687, Val Loss: 0.0247, Val Accuracy: 0.6344\n",
      "Epoch 14/25, Train Loss: 0.0228, Train Accuracy: 0.6725, Val Loss: 0.0249, Val Accuracy: 0.6272\n",
      "Epoch 15/25, Train Loss: 0.0224, Train Accuracy: 0.6835, Val Loss: 0.0255, Val Accuracy: 0.6507\n",
      "Epoch 16/25, Train Loss: 0.0226, Train Accuracy: 0.6769, Val Loss: 0.0256, Val Accuracy: 0.6329\n",
      "Epoch 17/25, Train Loss: 0.0221, Train Accuracy: 0.6847, Val Loss: 0.0253, Val Accuracy: 0.6550\n",
      "Epoch 18/25, Train Loss: 0.0219, Train Accuracy: 0.6843, Val Loss: 0.0253, Val Accuracy: 0.6408\n",
      "Epoch 19/25, Train Loss: 0.0216, Train Accuracy: 0.6933, Val Loss: 0.0265, Val Accuracy: 0.6251\n",
      "Epoch 20/25, Train Loss: 0.0219, Train Accuracy: 0.6911, Val Loss: 0.0252, Val Accuracy: 0.6450\n",
      "Epoch 21/25, Train Loss: 0.0213, Train Accuracy: 0.7009, Val Loss: 0.0262, Val Accuracy: 0.6130\n",
      "Epoch 22/25, Train Loss: 0.0213, Train Accuracy: 0.7014, Val Loss: 0.0268, Val Accuracy: 0.6436\n",
      "Epoch 23/25, Train Loss: 0.0211, Train Accuracy: 0.6959, Val Loss: 0.0266, Val Accuracy: 0.6344\n",
      "Epoch 24/25, Train Loss: 0.0208, Train Accuracy: 0.7045, Val Loss: 0.0266, Val Accuracy: 0.6201\n",
      "Epoch 25/25, Train Loss: 0.0207, Train Accuracy: 0.7038, Val Loss: 0.0259, Val Accuracy: 0.6515\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3 \n",
    "cc5_TE0_VE4_r_04 = CombinedClassifier5(num_classes=num_classes,dropout_rate=0.4).to(device)\n",
    "modelName = 'cc5_TE0_VE4_r_04'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainer6 = ModelTrainer(cc5_TE0_VE4_r_04, train_dataset_512, validate_dataset_512, modelName, epochs=25, save_interval=3)\n",
    "trainer6.train(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2a7e7-e6f4-4889-9a81-8449d54f7864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b520574346c3a8cee84a775377775649dc3260b2c2160397c14ffe3df1aca728"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
