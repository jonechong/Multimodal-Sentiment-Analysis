{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661414f9-aaad-427d-8643-a9286d45f5e5",
   "metadata": {},
   "source": [
    "# Single Modality AutoEncoder ( V + A )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8402c-5021-4675-ab27-bc0d1c1501a9",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3028fcb-3dc9-436c-b538-01164fedbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8a1b4-0e4e-405e-922d-9c292f3bff9f",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd91d9d2-34ec-4378-8f13-7b0f34dbcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, file_path, bert_feature_size='bert_text_features_128', split='train', dtype=torch.float32):\n",
    "        self.file_path = file_path\n",
    "        self.bert_feature_size = bert_feature_size\n",
    "        self.split = split\n",
    "        self.dtype = dtype \n",
    "        self.data_keys = []\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    " \n",
    "            for key in file.keys():\n",
    "                if file[key].attrs['split'] == self.split:\n",
    "                    self.data_keys.append(key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "\n",
    "            group_key = self.data_keys[idx]\n",
    "            group = file[group_key]\n",
    "\n",
    "\n",
    "            label = group.attrs['label']\n",
    "            text = group.attrs['text']\n",
    "            audio_features = torch.from_numpy(group['audio_features_averaged'][()]).type(self.dtype)\n",
    "            facial_features = torch.from_numpy(group['averaged_facial_features'][()]).type(self.dtype)\n",
    "            bert_features = torch.from_numpy(group[self.bert_feature_size][()]).type(self.dtype)\n",
    "\n",
    "\n",
    "            label_to_index = {'Positive': 2, 'Neutral': 1, 'Negative': 0}\n",
    "            label_index = label_to_index[label]\n",
    "\n",
    "\n",
    "            sample = {\n",
    "                'label': label_index,\n",
    "                'text': text,\n",
    "                'audio_features': audio_features,\n",
    "                'facial_features': facial_features,\n",
    "                'bert_features': bert_features\n",
    "            }\n",
    "\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b2ebea-3ab3-44e6-88e2-a8766cf079ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './combined_features.h5'\n",
    "datasets = {}\n",
    "bert_feature_sizes = ['bert_text_features_128', 'bert_text_features_256', 'bert_text_features_512']\n",
    "splits = ['train', 'validate', 'test']\n",
    "\n",
    "for feature_size in bert_feature_sizes:\n",
    "    datasets[feature_size] = {}\n",
    "    for split in splits:\n",
    "        dataset_key = f\"{split}_{feature_size}\"\n",
    "        datasets[feature_size][split] = CombinedDataset(file_path, bert_feature_size=feature_size, split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5405aa-4908-4e3c-9f4a-bfe915f7e58c",
   "metadata": {},
   "source": [
    "### Common Trainer Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8732a673-96f9-45a1-a2c5-35f08e17bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_dataset, val_dataset, model_name, epochs, save_interval, lr=1e-3, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.model_name = model_name\n",
    "        self.start_epoch = 0\n",
    "        self.epochs = epochs\n",
    "        self.save_interval = save_interval\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.history = defaultdict(list)\n",
    "        self.checkpoint_dir = f'modelCheckPoints/{self.model_name}'\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def save_checkpoint(self, epoch):\n",
    "        state = {'epoch': epoch, 'state_dict': self.model.state_dict()}\n",
    "        torch.save(state, f'{self.checkpoint_dir}/{epoch}.pt')\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        checkpoints = [ckpt for ckpt in os.listdir(self.checkpoint_dir) if ckpt.endswith('.pt')]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('.')[0]))\n",
    "            checkpoint = torch.load(f'{self.checkpoint_dir}/{latest_checkpoint}', map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.start_epoch = checkpoint['epoch'] + 1  \n",
    "            print(f\"Loaded checkpoint: {latest_checkpoint} at epoch {checkpoint['epoch']}\")\n",
    "        else:\n",
    "            self.start_epoch = 0  \n",
    "            print(\"No checkpoints found, starting from scratch.\")\n",
    "\n",
    "    def save_history(self):\n",
    "        with open(f'{self.checkpoint_dir}/history.json', 'w') as f:\n",
    "            json.dump(self.history, f)\n",
    "            \n",
    "    def initialize_optimizer(self):\n",
    "        sample_batch = next(iter(DataLoader(self.train_dataset, batch_size=1, shuffle=True)))\n",
    "        facial_features = sample_batch['facial_features'].to(self.device)\n",
    "        audio_features = sample_batch['audio_features'].to(self.device)\n",
    "        _ = self.model(facial_features, audio_features)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train_one_epoch(self, dataloader, criterion):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            facial_features = batch['facial_features'].to(self.device)\n",
    "            audio_features = batch['audio_features'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(facial_features, audio_features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        accuracy = correct_predictions / len(dataloader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def validate(self, dataloader, criterion):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                facial_features = batch['facial_features'].to(self.device)\n",
    "                audio_features = batch['audio_features'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "\n",
    "                outputs = self.model(facial_features, audio_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        accuracy = correct_predictions / len(dataloader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(self, criterion):\n",
    "        self.initialize_optimizer()\n",
    "        self.load_checkpoint()\n",
    "\n",
    "        train_dataloader = DataLoader(self.train_dataset, batch_size=128, shuffle=True)\n",
    "        val_dataloader = DataLoader(self.val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "        try:\n",
    "            for epoch in range(self.start_epoch, self.epochs):\n",
    "                train_loss, train_acc = self.train_one_epoch(train_dataloader, criterion)\n",
    "                val_loss, val_acc = self.validate(val_dataloader, criterion)\n",
    "\n",
    "                self.history['train_loss'].append(train_loss)\n",
    "                self.history['train_acc'].append(train_acc)\n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_acc'].append(val_acc)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, \"\n",
    "                      f\"Train Accuracy: {train_acc:.4f}, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, \"\n",
    "                      f\"Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "                if (epoch + 1) % self.save_interval == 0:\n",
    "                    self.save_checkpoint(epoch + 1)\n",
    "\n",
    "                self.save_history()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user. Saving last model state...\")\n",
    "            self.save_checkpoint(epoch + 1)\n",
    "            self.save_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c50ad-34f5-45ba-9843-5c2d67c7501b",
   "metadata": {},
   "source": [
    "# DynamicEncoder (Both modalities use the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c3ce8f7-dfdf-48e8-b9dc-f7ca32f10335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicEncoder(nn.Module):\n",
    "    def __init__(self, encoded_size=128, dropout_rate=0.5):\n",
    "        super(DynamicEncoder, self).__init__()\n",
    "        self.encoded_size = encoded_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.encoder is None:\n",
    "            input_size = x.size(1)\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_size, input_size // 2), \n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(input_size // 2, self.encoded_size),\n",
    "                nn.ReLU()\n",
    "            ).to(x.device)\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4179c552-859b-4670-b11a-68da8408d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedVideoAudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, encoded_audio_size=128, encoded_video_size=128, dropout_rate=0.5):\n",
    "        super(CombinedVideoAudioClassifier, self).__init__()\n",
    "        self.audio_encoder = DynamicEncoder(encoded_size=encoded_audio_size, dropout_rate=dropout_rate)\n",
    "        self.video_encoder = DynamicEncoder(encoded_size=encoded_video_size, dropout_rate=dropout_rate)\n",
    "        self.classifier = None\n",
    "        self.encoded_audio_size = encoded_audio_size\n",
    "        self.encoded_video_size = encoded_video_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, video_features, audio_features):\n",
    "        encoded_audio = self.audio_encoder(audio_features)\n",
    "        encoded_video = self.video_encoder(video_features)\n",
    "        \n",
    "        if self.classifier is None:\n",
    "            combined_feature_size = self.encoded_audio_size + self.encoded_video_size\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(combined_feature_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(512, self.num_classes)\n",
    "            ).to(audio_features.device)\n",
    "\n",
    "        combined_features = torch.cat((encoded_audio, encoded_video), dim=1)\n",
    "        return self.classifier(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a66b75-d0f0-4335-aec8-0e7665a6745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, starting from scratch.\n",
      "Epoch 1/25, Train Loss: 0.0111, Train Accuracy: 0.3483, Val Loss: 0.0086, Val Accuracy: 0.3870\n",
      "Epoch 2/25, Train Loss: 0.0085, Train Accuracy: 0.4208, Val Loss: 0.0083, Val Accuracy: 0.4319\n",
      "Epoch 3/25, Train Loss: 0.0083, Train Accuracy: 0.4421, Val Loss: 0.0082, Val Accuracy: 0.4448\n",
      "Epoch 4/25, Train Loss: 0.0083, Train Accuracy: 0.4519, Val Loss: 0.0081, Val Accuracy: 0.4490\n",
      "Epoch 5/25, Train Loss: 0.0082, Train Accuracy: 0.4657, Val Loss: 0.0081, Val Accuracy: 0.4661\n",
      "Epoch 6/25, Train Loss: 0.0081, Train Accuracy: 0.4677, Val Loss: 0.0081, Val Accuracy: 0.4597\n",
      "Epoch 7/25, Train Loss: 0.0081, Train Accuracy: 0.4689, Val Loss: 0.0081, Val Accuracy: 0.4469\n",
      "Epoch 8/25, Train Loss: 0.0080, Train Accuracy: 0.4881, Val Loss: 0.0081, Val Accuracy: 0.4547\n",
      "Epoch 9/25, Train Loss: 0.0079, Train Accuracy: 0.4858, Val Loss: 0.0080, Val Accuracy: 0.4590\n",
      "Epoch 10/25, Train Loss: 0.0079, Train Accuracy: 0.4962, Val Loss: 0.0079, Val Accuracy: 0.4761\n",
      "Epoch 11/25, Train Loss: 0.0078, Train Accuracy: 0.5028, Val Loss: 0.0079, Val Accuracy: 0.4697\n",
      "Epoch 12/25, Train Loss: 0.0078, Train Accuracy: 0.5079, Val Loss: 0.0080, Val Accuracy: 0.4690\n",
      "Epoch 13/25, Train Loss: 0.0078, Train Accuracy: 0.5065, Val Loss: 0.0079, Val Accuracy: 0.4683\n",
      "Epoch 14/25, Train Loss: 0.0077, Train Accuracy: 0.5148, Val Loss: 0.0079, Val Accuracy: 0.4768\n",
      "Epoch 15/25, Train Loss: 0.0077, Train Accuracy: 0.5172, Val Loss: 0.0079, Val Accuracy: 0.4861\n",
      "Epoch 16/25, Train Loss: 0.0076, Train Accuracy: 0.5202, Val Loss: 0.0079, Val Accuracy: 0.4775\n",
      "Epoch 17/25, Train Loss: 0.0075, Train Accuracy: 0.5373, Val Loss: 0.0081, Val Accuracy: 0.4640\n",
      "Epoch 18/25, Train Loss: 0.0075, Train Accuracy: 0.5318, Val Loss: 0.0080, Val Accuracy: 0.4697\n",
      "Epoch 19/25, Train Loss: 0.0075, Train Accuracy: 0.5359, Val Loss: 0.0078, Val Accuracy: 0.4904\n",
      "Epoch 20/25, Train Loss: 0.0074, Train Accuracy: 0.5414, Val Loss: 0.0079, Val Accuracy: 0.4854\n",
      "Epoch 21/25, Train Loss: 0.0075, Train Accuracy: 0.5360, Val Loss: 0.0079, Val Accuracy: 0.4989\n",
      "Epoch 22/25, Train Loss: 0.0074, Train Accuracy: 0.5429, Val Loss: 0.0079, Val Accuracy: 0.4847\n",
      "Epoch 23/25, Train Loss: 0.0074, Train Accuracy: 0.5475, Val Loss: 0.0079, Val Accuracy: 0.4890\n",
      "Epoch 24/25, Train Loss: 0.0073, Train Accuracy: 0.5543, Val Loss: 0.0079, Val Accuracy: 0.4890\n",
      "Epoch 25/25, Train Loss: 0.0072, Train Accuracy: 0.5582, Val Loss: 0.0080, Val Accuracy: 0.4875\n"
     ]
    }
   ],
   "source": [
    "train_dataset_512 = datasets['bert_text_features_512']['train']\n",
    "validate_dataset_512 = datasets['bert_text_features_512']['validate']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_classes = 3  \n",
    "bert_feature_size = 512\n",
    "CombinedClassifier_A1_V1 = CombinedVideoAudioClassifier(bert_feature_size, num_classes,).to(device)\n",
    "modelName = 'CombinedClassifier_A1_V1'\n",
    "\n",
    "trainer = ModelTrainer(CombinedClassifier_A1_V1, train_dataset_512, validate_dataset_512, modelName, epochs=25, save_interval=5, device=device)\n",
    "trainer.train(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e930ac8-933b-4a0b-a294-195e22d32ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
