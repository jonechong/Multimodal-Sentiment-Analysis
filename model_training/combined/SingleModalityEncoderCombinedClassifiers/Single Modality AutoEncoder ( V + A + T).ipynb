{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661414f9-aaad-427d-8643-a9286d45f5e5",
   "metadata": {},
   "source": [
    "# Single Modality AutoEncoder ( V + A + T )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8402c-5021-4675-ab27-bc0d1c1501a9",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3028fcb-3dc9-436c-b538-01164fedbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8a1b4-0e4e-405e-922d-9c292f3bff9f",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd91d9d2-34ec-4378-8f13-7b0f34dbcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, file_path, bert_feature_size='bert_text_features_128', split='train', dtype=torch.float32):\n",
    "        self.file_path = file_path\n",
    "        self.bert_feature_size = bert_feature_size\n",
    "        self.split = split\n",
    "        self.dtype = dtype  # Store the desired data type\n",
    "        self.data_keys = []\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            # Iterate through groups in HDF5 file and store keys for the specified split\n",
    "            for key in file.keys():\n",
    "                if file[key].attrs['split'] == self.split:\n",
    "                    self.data_keys.append(key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            # Access the group corresponding to the index\n",
    "            group_key = self.data_keys[idx]\n",
    "            group = file[group_key]\n",
    "\n",
    "            # Read datasets from the group and convert them to the specified dtype\n",
    "            label = group.attrs['label']\n",
    "            text = group.attrs['text']\n",
    "            audio_features = torch.from_numpy(group['audio_features_averaged'][()]).type(self.dtype)\n",
    "            facial_features = torch.from_numpy(group['averaged_facial_features'][()]).type(self.dtype)\n",
    "            bert_features = torch.from_numpy(group[self.bert_feature_size][()]).type(self.dtype)\n",
    "\n",
    "            # Convert label to a numeric format, if necessary\n",
    "            label_to_index = {'Positive': 2, 'Neutral': 1, 'Negative': 0}\n",
    "            label_index = label_to_index[label]\n",
    "\n",
    "            # Create a dictionary with the data\n",
    "            sample = {\n",
    "                'label': label_index,\n",
    "                'text': text,\n",
    "                'audio_features': audio_features,\n",
    "                'facial_features': facial_features,\n",
    "                'bert_features': bert_features\n",
    "            }\n",
    "\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b2ebea-3ab3-44e6-88e2-a8766cf079ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the HDF5 file for each BERT feature size version\n",
    "file_path = './combined_features.h5'\n",
    "\n",
    "# Create a CombinedDataset for each BERT feature size and split\n",
    "datasets = {}\n",
    "bert_feature_sizes = ['bert_text_features_128', 'bert_text_features_256', 'bert_text_features_512']\n",
    "splits = ['train', 'validate', 'test']\n",
    "\n",
    "for feature_size in bert_feature_sizes:\n",
    "    datasets[feature_size] = {}\n",
    "    for split in splits:\n",
    "        dataset_key = f\"{split}_{feature_size}\"\n",
    "        datasets[feature_size][split] = CombinedDataset(file_path, bert_feature_size=feature_size, split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5405aa-4908-4e3c-9f4a-bfe915f7e58c",
   "metadata": {},
   "source": [
    "### Common Trainer Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8732a673-96f9-45a1-a2c5-35f08e17bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_dataset, val_dataset, model_name, epochs, save_interval, lr=1e-3, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.model_name = model_name\n",
    "        self.start_epoch = 0\n",
    "        self.epochs = epochs\n",
    "        self.save_interval = save_interval\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.history = defaultdict(list)\n",
    "        self.checkpoint_dir = f'modelCheckPoints/{self.model_name}'\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        self.optimizer = None  # Initialized later in initialize_optimizer()\n",
    "        \n",
    "    def save_checkpoint(self, epoch):\n",
    "        # Save model parameters along with the epoch number\n",
    "        state = {'epoch': epoch, 'state_dict': self.model.state_dict()}\n",
    "        torch.save(state, f'{self.checkpoint_dir}/{epoch}.pt')\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        # Load the latest model checkpoint\n",
    "        checkpoints = [ckpt for ckpt in os.listdir(self.checkpoint_dir) if ckpt.endswith('.pt')]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('.')[0]))\n",
    "            checkpoint = torch.load(f'{self.checkpoint_dir}/{latest_checkpoint}', map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.start_epoch = checkpoint['epoch'] + 1  # Continue from next epoch\n",
    "            print(f\"Loaded checkpoint: {latest_checkpoint} at epoch {checkpoint['epoch']}\")\n",
    "        else:\n",
    "            self.start_epoch = 0  # Start from scratch\n",
    "            print(\"No checkpoints found, starting from scratch.\")\n",
    "\n",
    "    def save_history(self):\n",
    "        # Save history of performance\n",
    "        with open(f'{self.checkpoint_dir}/history.json', 'w') as f:\n",
    "            json.dump(self.history, f)\n",
    "            \n",
    "    def initialize_optimizer(self):\n",
    "        # Perform a dummy forward pass to initialize model parameters\n",
    "        sample_batch = next(iter(DataLoader(self.train_dataset, batch_size=1, shuffle=True)))\n",
    "        facial_features = sample_batch['facial_features'].to(self.device)\n",
    "        audio_features = sample_batch['audio_features'].to(self.device)\n",
    "        bert_features = sample_batch['bert_features'].to(self.device)\n",
    "\n",
    "        # Assuming the model's forward method expects bert_features and audio_features \n",
    "        _ = self.model(audio_features, facial_features, bert_features)\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train_one_epoch(self, dataloader, criterion):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            facial_features = batch['facial_features'].to(self.device)\n",
    "            audio_features = batch['audio_features'].to(self.device)\n",
    "            bert_features = batch['bert_features'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(audio_features, facial_features, bert_features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        accuracy = correct_predictions / len(dataloader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def validate(self, dataloader, criterion):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                facial_features = batch['facial_features'].to(self.device)\n",
    "                audio_features = batch['audio_features'].to(self.device)\n",
    "                bert_features = batch['bert_features'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "\n",
    "                outputs = self.model(audio_features, facial_features, bert_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        accuracy = correct_predictions / len(dataloader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(self, criterion):\n",
    "        self.initialize_optimizer()\n",
    "        self.load_checkpoint()\n",
    "\n",
    "        train_dataloader = DataLoader(self.train_dataset, batch_size=128, shuffle=True)\n",
    "        val_dataloader = DataLoader(self.val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "        try:\n",
    "            for epoch in range(self.start_epoch, self.epochs):\n",
    "                train_loss, train_acc = self.train_one_epoch(train_dataloader, criterion)\n",
    "                val_loss, val_acc = self.validate(val_dataloader, criterion)\n",
    "\n",
    "                self.history['train_loss'].append(train_loss)\n",
    "                self.history['train_acc'].append(train_acc)\n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_acc'].append(val_acc)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, \"\n",
    "                      f\"Train Accuracy: {train_acc:.4f}, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, \"\n",
    "                      f\"Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "                if (epoch + 1) % self.save_interval == 0:\n",
    "                    self.save_checkpoint(epoch + 1)\n",
    "\n",
    "                self.save_history()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user. Saving last model state...\")\n",
    "            self.save_checkpoint(epoch + 1)\n",
    "            self.save_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c50ad-34f5-45ba-9843-5c2d67c7501b",
   "metadata": {},
   "source": [
    "# DynamicEncoder (T + V + A all modalities use the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3ce8f7-dfdf-48e8-b9dc-f7ca32f10335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicEncoder(nn.Module):\n",
    "    def __init__(self, encoded_size=128, dropout_rate=0.5):\n",
    "        super(DynamicEncoder, self).__init__()\n",
    "        self.encoded_size = encoded_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.encoder is None:\n",
    "            input_size = x.size(1)\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_size, input_size // 2),  # Example of dynamic layer sizing\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(input_size // 2, self.encoded_size),\n",
    "                nn.ReLU()\n",
    "            ).to(x.device)\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4179c552-859b-4670-b11a-68da8408d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, encoded_audio_size=128, encoded_video_size=128, encoded_text_size=128, dropout_rate=0.5):\n",
    "        super(CombinedClassifier, self).__init__()\n",
    "        # Initialize encoders for each modality with their respective encoded sizes\n",
    "        self.audio_encoder = DynamicEncoder(encoded_size=encoded_audio_size, dropout_rate=dropout_rate)\n",
    "        self.video_encoder = DynamicEncoder(encoded_size=encoded_video_size, dropout_rate=dropout_rate)\n",
    "        self.text_encoder = DynamicEncoder(encoded_size=encoded_text_size, dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = None  # This will be dynamically created\n",
    "\n",
    "    def forward(self, audio_features, video_features, text_features):\n",
    "        encoded_audio = self.audio_encoder(audio_features)\n",
    "        encoded_video = self.video_encoder(video_features)\n",
    "        encoded_text = self.text_encoder(text_features)\n",
    "\n",
    "        # Initialize the classifier dynamically based on the combined feature size\n",
    "        if self.classifier is None:\n",
    "            combined_feature_size = encoded_audio.size(1) + encoded_video.size(1) + encoded_text.size(1)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(combined_feature_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(512, self.num_classes)\n",
    "            ).to(audio_features.device)  # Assume all features are on the same device\n",
    "\n",
    "        combined_features = torch.cat((encoded_audio, encoded_video, encoded_text), dim=1)\n",
    "        return self.classifier(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a66b75-d0f0-4335-aec8-0e7665a6745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, starting from scratch.\n",
      "Epoch 1/25, Train Loss: 0.0073, Train Accuracy: 0.5437, Val Loss: 0.0063, Val Accuracy: 0.6265\n",
      "Epoch 2/25, Train Loss: 0.0064, Train Accuracy: 0.6277, Val Loss: 0.0062, Val Accuracy: 0.6315\n",
      "Epoch 3/25, Train Loss: 0.0062, Train Accuracy: 0.6433, Val Loss: 0.0061, Val Accuracy: 0.6458\n",
      "Epoch 4/25, Train Loss: 0.0061, Train Accuracy: 0.6524, Val Loss: 0.0061, Val Accuracy: 0.6422\n",
      "Epoch 5/25, Train Loss: 0.0060, Train Accuracy: 0.6601, Val Loss: 0.0061, Val Accuracy: 0.6500\n",
      "Epoch 6/25, Train Loss: 0.0059, Train Accuracy: 0.6702, Val Loss: 0.0060, Val Accuracy: 0.6429\n",
      "Epoch 7/25, Train Loss: 0.0058, Train Accuracy: 0.6781, Val Loss: 0.0061, Val Accuracy: 0.6522\n",
      "Epoch 8/25, Train Loss: 0.0058, Train Accuracy: 0.6781, Val Loss: 0.0059, Val Accuracy: 0.6436\n",
      "Epoch 9/25, Train Loss: 0.0056, Train Accuracy: 0.6874, Val Loss: 0.0061, Val Accuracy: 0.6543\n",
      "Epoch 10/25, Train Loss: 0.0055, Train Accuracy: 0.6919, Val Loss: 0.0062, Val Accuracy: 0.6322\n",
      "Epoch 11/25, Train Loss: 0.0054, Train Accuracy: 0.6989, Val Loss: 0.0060, Val Accuracy: 0.6593\n",
      "Epoch 12/25, Train Loss: 0.0054, Train Accuracy: 0.7006, Val Loss: 0.0059, Val Accuracy: 0.6522\n",
      "Epoch 13/25, Train Loss: 0.0052, Train Accuracy: 0.7128, Val Loss: 0.0061, Val Accuracy: 0.6386\n",
      "Epoch 14/25, Train Loss: 0.0051, Train Accuracy: 0.7199, Val Loss: 0.0061, Val Accuracy: 0.6415\n",
      "Epoch 15/25, Train Loss: 0.0050, Train Accuracy: 0.7215, Val Loss: 0.0060, Val Accuracy: 0.6515\n",
      "Epoch 16/25, Train Loss: 0.0050, Train Accuracy: 0.7295, Val Loss: 0.0060, Val Accuracy: 0.6607\n",
      "Epoch 17/25, Train Loss: 0.0048, Train Accuracy: 0.7441, Val Loss: 0.0063, Val Accuracy: 0.6536\n",
      "Epoch 18/25, Train Loss: 0.0047, Train Accuracy: 0.7459, Val Loss: 0.0062, Val Accuracy: 0.6443\n",
      "Epoch 19/25, Train Loss: 0.0046, Train Accuracy: 0.7514, Val Loss: 0.0063, Val Accuracy: 0.6465\n",
      "Epoch 20/25, Train Loss: 0.0046, Train Accuracy: 0.7553, Val Loss: 0.0064, Val Accuracy: 0.6429\n",
      "Epoch 21/25, Train Loss: 0.0044, Train Accuracy: 0.7685, Val Loss: 0.0067, Val Accuracy: 0.6329\n",
      "Epoch 22/25, Train Loss: 0.0043, Train Accuracy: 0.7714, Val Loss: 0.0065, Val Accuracy: 0.6415\n",
      "Epoch 23/25, Train Loss: 0.0042, Train Accuracy: 0.7745, Val Loss: 0.0066, Val Accuracy: 0.6436\n",
      "Epoch 24/25, Train Loss: 0.0040, Train Accuracy: 0.7868, Val Loss: 0.0066, Val Accuracy: 0.6486\n",
      "Epoch 25/25, Train Loss: 0.0039, Train Accuracy: 0.7918, Val Loss: 0.0066, Val Accuracy: 0.6436\n"
     ]
    }
   ],
   "source": [
    "# Initialize datasets for 'bert_text_features_512'\n",
    "train_dataset_512 = datasets['bert_text_features_512']['train']\n",
    "validate_dataset_512 = datasets['bert_text_features_512']['validate']\n",
    "# test_dataset_512 = datasets['bert_text_features_512']['test']  # for evaluation later\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 3  # The number of classes in your classification task\n",
    "CombinedClassifier_A1_V1_T1 = CombinedClassifier(num_classes, encoded_audio_size=128, encoded_video_size=128, encoded_text_size=128, dropout_rate=0.5).to(device)\n",
    "modelName = 'CombinedClassifier_A1_V1_T1'\n",
    "\n",
    "# Define the trainer\n",
    "trainer = ModelTrainer(CombinedClassifier_A1_V1_T1, train_dataset_512, validate_dataset_512, modelName, epochs=25, save_interval=5, device=device)\n",
    "\n",
    "# Start training\n",
    "trainer.train(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4e2441-913b-4b92-95c9-2edaa8a3f24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, starting from scratch.\n",
      "Epoch 1/25, Train Loss: 0.0077, Train Accuracy: 0.5001, Val Loss: 0.0065, Val Accuracy: 0.6123\n",
      "Epoch 2/25, Train Loss: 0.0068, Train Accuracy: 0.6018, Val Loss: 0.0064, Val Accuracy: 0.6151\n",
      "Epoch 3/25, Train Loss: 0.0065, Train Accuracy: 0.6238, Val Loss: 0.0061, Val Accuracy: 0.6450\n",
      "Epoch 4/25, Train Loss: 0.0064, Train Accuracy: 0.6273, Val Loss: 0.0062, Val Accuracy: 0.6443\n",
      "Epoch 5/25, Train Loss: 0.0064, Train Accuracy: 0.6335, Val Loss: 0.0060, Val Accuracy: 0.6344\n",
      "Epoch 6/25, Train Loss: 0.0063, Train Accuracy: 0.6417, Val Loss: 0.0060, Val Accuracy: 0.6515\n",
      "Epoch 7/25, Train Loss: 0.0062, Train Accuracy: 0.6475, Val Loss: 0.0060, Val Accuracy: 0.6479\n",
      "Epoch 8/25, Train Loss: 0.0062, Train Accuracy: 0.6462, Val Loss: 0.0061, Val Accuracy: 0.6450\n",
      "Epoch 9/25, Train Loss: 0.0061, Train Accuracy: 0.6546, Val Loss: 0.0060, Val Accuracy: 0.6550\n",
      "Epoch 10/25, Train Loss: 0.0061, Train Accuracy: 0.6592, Val Loss: 0.0060, Val Accuracy: 0.6450\n",
      "Epoch 11/25, Train Loss: 0.0061, Train Accuracy: 0.6600, Val Loss: 0.0060, Val Accuracy: 0.6450\n",
      "Epoch 12/25, Train Loss: 0.0060, Train Accuracy: 0.6545, Val Loss: 0.0061, Val Accuracy: 0.6315\n",
      "Epoch 13/25, Train Loss: 0.0059, Train Accuracy: 0.6642, Val Loss: 0.0059, Val Accuracy: 0.6543\n",
      "Epoch 14/25, Train Loss: 0.0059, Train Accuracy: 0.6645, Val Loss: 0.0059, Val Accuracy: 0.6565\n",
      "Epoch 15/25, Train Loss: 0.0059, Train Accuracy: 0.6707, Val Loss: 0.0059, Val Accuracy: 0.6543\n",
      "Epoch 16/25, Train Loss: 0.0058, Train Accuracy: 0.6756, Val Loss: 0.0059, Val Accuracy: 0.6529\n",
      "Epoch 17/25, Train Loss: 0.0058, Train Accuracy: 0.6765, Val Loss: 0.0060, Val Accuracy: 0.6422\n",
      "Epoch 18/25, Train Loss: 0.0057, Train Accuracy: 0.6801, Val Loss: 0.0059, Val Accuracy: 0.6522\n",
      "Epoch 19/25, Train Loss: 0.0057, Train Accuracy: 0.6830, Val Loss: 0.0060, Val Accuracy: 0.6557\n",
      "Epoch 20/25, Train Loss: 0.0056, Train Accuracy: 0.6842, Val Loss: 0.0059, Val Accuracy: 0.6557\n",
      "Epoch 21/25, Train Loss: 0.0056, Train Accuracy: 0.6859, Val Loss: 0.0059, Val Accuracy: 0.6536\n",
      "Epoch 22/25, Train Loss: 0.0055, Train Accuracy: 0.6864, Val Loss: 0.0058, Val Accuracy: 0.6586\n",
      "Epoch 23/25, Train Loss: 0.0055, Train Accuracy: 0.6928, Val Loss: 0.0059, Val Accuracy: 0.6550\n",
      "Epoch 24/25, Train Loss: 0.0054, Train Accuracy: 0.6999, Val Loss: 0.0059, Val Accuracy: 0.6586\n",
      "Epoch 25/25, Train Loss: 0.0053, Train Accuracy: 0.7013, Val Loss: 0.0059, Val Accuracy: 0.6486\n"
     ]
    }
   ],
   "source": [
    "# Initialize datasets for 'bert_text_features_512'\n",
    "train_dataset_512 = datasets['bert_text_features_512']['train']\n",
    "validate_dataset_512 = datasets['bert_text_features_512']['validate']\n",
    "# test_dataset_512 = datasets['bert_text_features_512']['test']  # for evaluation later\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 3  # The number of classes in your classification task\n",
    "CombinedClassifier_A1_V1_T1_r1 = CombinedClassifier(num_classes, encoded_audio_size=128, encoded_video_size=128, encoded_text_size=128, dropout_rate=0.7).to(device)\n",
    "modelName = 'CombinedClassifier_A1_V1_T1_r1'\n",
    "\n",
    "# Define the trainer\n",
    "trainer1 = ModelTrainer(CombinedClassifier_A1_V1_T1_r1, train_dataset_512, validate_dataset_512, modelName, epochs=25, save_interval=5, device=device)\n",
    "\n",
    "# Start training\n",
    "trainer1.train(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eef2cec-f209-4521-8a45-76f199458612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, starting from scratch.\n",
      "Epoch 1/30, Train Loss: 0.0072, Train Accuracy: 0.5531, Val Loss: 0.0063, Val Accuracy: 0.6408\n",
      "Epoch 2/30, Train Loss: 0.0063, Train Accuracy: 0.6378, Val Loss: 0.0063, Val Accuracy: 0.6272\n",
      "Epoch 3/30, Train Loss: 0.0062, Train Accuracy: 0.6427, Val Loss: 0.0061, Val Accuracy: 0.6486\n",
      "Epoch 4/30, Train Loss: 0.0060, Train Accuracy: 0.6566, Val Loss: 0.0061, Val Accuracy: 0.6344\n",
      "Epoch 5/30, Train Loss: 0.0059, Train Accuracy: 0.6663, Val Loss: 0.0062, Val Accuracy: 0.6393\n",
      "Epoch 6/30, Train Loss: 0.0058, Train Accuracy: 0.6745, Val Loss: 0.0059, Val Accuracy: 0.6536\n",
      "Epoch 7/30, Train Loss: 0.0056, Train Accuracy: 0.6844, Val Loss: 0.0060, Val Accuracy: 0.6600\n",
      "Epoch 8/30, Train Loss: 0.0055, Train Accuracy: 0.6944, Val Loss: 0.0060, Val Accuracy: 0.6600\n",
      "Epoch 9/30, Train Loss: 0.0054, Train Accuracy: 0.7030, Val Loss: 0.0058, Val Accuracy: 0.6636\n",
      "Epoch 10/30, Train Loss: 0.0053, Train Accuracy: 0.7138, Val Loss: 0.0060, Val Accuracy: 0.6372\n",
      "Epoch 11/30, Train Loss: 0.0051, Train Accuracy: 0.7162, Val Loss: 0.0061, Val Accuracy: 0.6493\n",
      "Epoch 12/30, Train Loss: 0.0051, Train Accuracy: 0.7195, Val Loss: 0.0061, Val Accuracy: 0.6572\n",
      "Epoch 13/30, Train Loss: 0.0048, Train Accuracy: 0.7337, Val Loss: 0.0063, Val Accuracy: 0.6458\n",
      "Epoch 14/30, Train Loss: 0.0048, Train Accuracy: 0.7433, Val Loss: 0.0063, Val Accuracy: 0.6408\n",
      "Epoch 15/30, Train Loss: 0.0046, Train Accuracy: 0.7519, Val Loss: 0.0061, Val Accuracy: 0.6500\n",
      "Epoch 16/30, Train Loss: 0.0045, Train Accuracy: 0.7574, Val Loss: 0.0062, Val Accuracy: 0.6507\n",
      "Epoch 17/30, Train Loss: 0.0044, Train Accuracy: 0.7604, Val Loss: 0.0063, Val Accuracy: 0.6529\n",
      "Epoch 18/30, Train Loss: 0.0042, Train Accuracy: 0.7749, Val Loss: 0.0066, Val Accuracy: 0.6358\n",
      "Epoch 19/30, Train Loss: 0.0040, Train Accuracy: 0.7891, Val Loss: 0.0065, Val Accuracy: 0.6472\n",
      "Epoch 20/30, Train Loss: 0.0040, Train Accuracy: 0.7830, Val Loss: 0.0066, Val Accuracy: 0.6636\n",
      "Epoch 21/30, Train Loss: 0.0038, Train Accuracy: 0.8008, Val Loss: 0.0067, Val Accuracy: 0.6458\n",
      "Epoch 22/30, Train Loss: 0.0038, Train Accuracy: 0.8045, Val Loss: 0.0068, Val Accuracy: 0.6536\n",
      "Epoch 23/30, Train Loss: 0.0036, Train Accuracy: 0.8101, Val Loss: 0.0071, Val Accuracy: 0.6393\n",
      "Epoch 24/30, Train Loss: 0.0035, Train Accuracy: 0.8173, Val Loss: 0.0077, Val Accuracy: 0.6408\n",
      "Epoch 25/30, Train Loss: 0.0034, Train Accuracy: 0.8171, Val Loss: 0.0076, Val Accuracy: 0.6294\n",
      "Epoch 26/30, Train Loss: 0.0032, Train Accuracy: 0.8288, Val Loss: 0.0076, Val Accuracy: 0.6386\n",
      "Epoch 27/30, Train Loss: 0.0032, Train Accuracy: 0.8313, Val Loss: 0.0081, Val Accuracy: 0.6401\n",
      "Epoch 28/30, Train Loss: 0.0029, Train Accuracy: 0.8461, Val Loss: 0.0079, Val Accuracy: 0.6429\n",
      "Epoch 29/30, Train Loss: 0.0029, Train Accuracy: 0.8530, Val Loss: 0.0083, Val Accuracy: 0.6358\n",
      "Epoch 30/30, Train Loss: 0.0027, Train Accuracy: 0.8588, Val Loss: 0.0085, Val Accuracy: 0.6358\n"
     ]
    }
   ],
   "source": [
    "# Initialize datasets for 'bert_text_features_512'\n",
    "train_dataset_512 = datasets['bert_text_features_512']['train']\n",
    "validate_dataset_512 = datasets['bert_text_features_512']['validate']\n",
    "# test_dataset_512 = datasets['bert_text_features_512']['test']  # for evaluation later\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 3  # The number of classes in your classification task\n",
    "CombinedClassifier_A2_V2_T1_r0 = CombinedClassifier(num_classes, encoded_audio_size=64, encoded_video_size=64, encoded_text_size=128, dropout_rate=0.4).to(device)\n",
    "modelName = 'CombinedClassifier_A2_V2_T1_r0'\n",
    "\n",
    "# Define the trainer\n",
    "trainer2 = ModelTrainer(CombinedClassifier_A2_V2_T1_r0, train_dataset_512, validate_dataset_512, modelName, epochs=30, save_interval=5, device=device)\n",
    "\n",
    "# Start training\n",
    "trainer2.train(criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
