{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd5ba9b-effd-436d-bd98-8f5d64352601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace 'path_to_your_file.pkl' with the actual file path of your .pkl file\n",
    "file_path = 'aligned_50.pkl'\n",
    "\n",
    "# Unpack the pickle file\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae00ce4-0ef1-4d22-a5c1-71bfe18ef5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader , WeightedRandomSampler\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd123c6d-aa04-484d-8aab-3b7b9de8352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.audio = torch.tensor(data['audio'])\n",
    "        self.vision = torch.tensor(data['vision'])\n",
    "        self.text_bert = torch.tensor(data['text_bert'])\n",
    "        # Map annotations to integers\n",
    "        label_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "        self.labels = torch.tensor([label_mapping[ann] for ann in data['annotations']])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'audio': self.audio[idx],\n",
    "            'vision': self.vision[idx],\n",
    "            'text_bert': self.text_bert[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "        return sample\n",
    "        \n",
    "def get_class_weights(dataset):\n",
    "    # Count the number of occurrences of each class\n",
    "    class_counts = Counter([dataset[i]['label'].item() for i in range(len(dataset))])\n",
    "    num_samples = sum(class_counts.values())\n",
    "    \n",
    "    # Calculate weight for each class\n",
    "    weights = {cls: num_samples / count for cls, count in class_counts.items()}\n",
    "    \n",
    "    # Assign weight to each sample in the dataset\n",
    "    sample_weights = [weights[dataset[i]['label'].item()] for i in range(len(dataset))]\n",
    "    \n",
    "    return sample_weights\n",
    "\n",
    "train_dataset = MultimodalDataset(data['train'])\n",
    "valid_dataset = MultimodalDataset(data['valid'])\n",
    "test_dataset = MultimodalDataset(data['test'])\n",
    "\n",
    "# Calculate weights for each sample in the training and validation datasets\n",
    "train_sample_weights = get_class_weights(train_dataset)\n",
    "val_sample_weights = get_class_weights(valid_dataset)\n",
    "\n",
    "# Create WeightedRandomSampler for training and validation datasets\n",
    "train_sampler = WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_sample_weights), replacement=True)\n",
    "val_sampler = WeightedRandomSampler(weights=val_sample_weights, num_samples=len(val_sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders with WeightedRandomSampler\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(valid_dataset, batch_size=256, sampler=val_sampler)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3521ca8e-76ed-47a6-94b3-d4a1b7630213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio tensor shape: torch.Size([256, 50, 74])\n",
      "Vision tensor shape: torch.Size([256, 50, 35])\n",
      "Text BERT tensor shape: torch.Size([256, 3, 50])\n",
      "Labels tensor shape: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_dataloader is already created and initialized\n",
    "# Let's fetch one batch from the DataLoader\n",
    "for batch in train_dataloader:\n",
    "    audio = batch['audio']\n",
    "    vision = batch['vision']\n",
    "    text_bert = batch['text_bert']\n",
    "    labels = batch['label']\n",
    "\n",
    "    print(f'Audio tensor shape: {audio.shape}')\n",
    "    print(f'Vision tensor shape: {vision.shape}')\n",
    "    print(f'Text BERT tensor shape: {text_bert.shape}')\n",
    "    print(f'Labels tensor shape: {labels.shape}')\n",
    "\n",
    "    # Break after the first batch to just see one sample of the data shapes\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca2e0a9d-d8bf-4204-97d2-5aef5d158548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_dataset, val_dataset, model_name, epochs, save_interval, lr=1e-3, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.model_name = model_name\n",
    "        self.start_epoch = 0\n",
    "        self.epochs = epochs\n",
    "        self.save_interval = save_interval\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.history = defaultdict(list)\n",
    "        self.checkpoint_dir = f'modelCheckPoints/{self.model_name}'\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        state = {'epoch': epoch, 'state_dict': self.model.state_dict()}\n",
    "        torch.save(state, f'{self.checkpoint_dir}/{epoch}.pt')\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        checkpoints = [ckpt for ckpt in os.listdir(self.checkpoint_dir) if ckpt.endswith('.pt')]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('.')[0]))\n",
    "            checkpoint = torch.load(f'{self.checkpoint_dir}/{latest_checkpoint}', map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Loaded checkpoint: {latest_checkpoint} at epoch {checkpoint['epoch']}\")\n",
    "        else:\n",
    "            print(\"No checkpoints found, starting from scratch.\")\n",
    "\n",
    "    def save_history(self):\n",
    "        with open(f'{self.checkpoint_dir}/history.json', 'w') as f:\n",
    "            json.dump(self.history, f)\n",
    "\n",
    "    def train_one_epoch(self, dataloader, criterion, max_grad_norm=1.0):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "    \n",
    "        for batch in dataloader:\n",
    "            audio = batch['audio'].to(self.device).float()\n",
    "            vision = batch['vision'].to(self.device).float()\n",
    "            text_bert = batch['text_bert'].to(self.device).float()\n",
    "            labels = batch['label'].to(self.device)\n",
    "    \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(audio, vision, text_bert)\n",
    "    \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        accuracy = correct_predictions / len(dataloader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def validate(self, dataloader, criterion):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                audio = batch['audio'].to(self.device).float()\n",
    "                vision = batch['vision'].to(self.device).float()\n",
    "                text_bert = batch['text_bert'].to(self.device).float()\n",
    "                labels = batch['label'].to(self.device)\n",
    "    \n",
    "                outputs = self.model(audio, vision, text_bert)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        accuracy = correct_predictions / len(dataloader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(self, criterion):\n",
    "        self.load_checkpoint()\n",
    "        train_dataloader = DataLoader(self.train_dataset, batch_size=256, shuffle=True)\n",
    "        val_dataloader = DataLoader(self.val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs):\n",
    "            train_loss, train_acc = self.train_one_epoch(train_dataloader, criterion)\n",
    "            val_loss, val_acc = self.validate(val_dataloader, criterion)\n",
    "    \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "    \n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "            if (epoch + 1) % self.save_interval == 0:\n",
    "                self.save_checkpoint(epoch + 1)\n",
    "    \n",
    "            self.save_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76ec48-673c-4566-8749-69899e7cf370",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d04ae73d-e9dd-441e-9d46-314375c7478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, audio_dim, vision_dim, text_dim, hidden_dim, num_classes):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "\n",
    "        self.audio_lstm = nn.LSTM(audio_dim, hidden_dim, batch_first=True)\n",
    "        self.vision_lstm = nn.LSTM(vision_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Adjust the text_fc layer to match the flattened text_bert dimensions\n",
    "        self.text_fc = nn.Linear(text_dim * 50, hidden_dim)  # Multiply by sequence length\n",
    "\n",
    "        self.combined_fc = nn.Linear(hidden_dim * 3, num_classes)\n",
    "\n",
    "    def forward(self, audio, vision, text_bert):\n",
    "        _, (audio_hidden, _) = self.audio_lstm(audio)\n",
    "        audio_repr = audio_hidden.squeeze(0)\n",
    "\n",
    "        _, (vision_hidden, _) = self.vision_lstm(vision)\n",
    "        vision_repr = vision_hidden.squeeze(0)\n",
    "\n",
    "        # Flatten the text_bert features before passing through the linear layer\n",
    "        text_flattened = text_bert.reshape(text_bert.size(0), -1)\n",
    "        text_repr = F.relu(self.text_fc(text_flattened))\n",
    "\n",
    "        combined_features = torch.cat((audio_repr, vision_repr, text_repr), dim=1)\n",
    "        output = self.combined_fc(combined_features)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c6d356-573a-4107-a78d-6d4a1b6daae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: 100.pt at epoch 100\n",
      "Epoch 102/200, Train Loss: 0.0014, Train Accuracy: 0.8583, Val Loss: 0.0102, Val Accuracy: 0.4254\n",
      "Epoch 103/200, Train Loss: 0.0013, Train Accuracy: 0.8690, Val Loss: 0.0108, Val Accuracy: 0.4313\n",
      "Epoch 104/200, Train Loss: 0.0013, Train Accuracy: 0.8716, Val Loss: 0.0108, Val Accuracy: 0.4367\n",
      "Epoch 105/200, Train Loss: 0.0012, Train Accuracy: 0.8755, Val Loss: 0.0112, Val Accuracy: 0.4196\n",
      "Epoch 106/200, Train Loss: 0.0012, Train Accuracy: 0.8828, Val Loss: 0.0118, Val Accuracy: 0.4415\n",
      "Epoch 107/200, Train Loss: 0.0012, Train Accuracy: 0.8807, Val Loss: 0.0114, Val Accuracy: 0.4105\n",
      "Epoch 108/200, Train Loss: 0.0012, Train Accuracy: 0.8791, Val Loss: 0.0114, Val Accuracy: 0.4303\n",
      "Epoch 109/200, Train Loss: 0.0012, Train Accuracy: 0.8842, Val Loss: 0.0117, Val Accuracy: 0.4270\n",
      "Epoch 110/200, Train Loss: 0.0011, Train Accuracy: 0.8948, Val Loss: 0.0118, Val Accuracy: 0.4249\n",
      "Epoch 111/200, Train Loss: 0.0010, Train Accuracy: 0.8956, Val Loss: 0.0123, Val Accuracy: 0.4297\n",
      "Epoch 112/200, Train Loss: 0.0011, Train Accuracy: 0.8923, Val Loss: 0.0117, Val Accuracy: 0.4420\n",
      "Epoch 113/200, Train Loss: 0.0011, Train Accuracy: 0.8967, Val Loss: 0.0123, Val Accuracy: 0.4238\n",
      "Epoch 114/200, Train Loss: 0.0010, Train Accuracy: 0.8993, Val Loss: 0.0127, Val Accuracy: 0.4313\n",
      "Epoch 115/200, Train Loss: 0.0010, Train Accuracy: 0.8970, Val Loss: 0.0128, Val Accuracy: 0.4158\n",
      "Epoch 116/200, Train Loss: 0.0010, Train Accuracy: 0.9041, Val Loss: 0.0127, Val Accuracy: 0.4180\n",
      "Epoch 117/200, Train Loss: 0.0009, Train Accuracy: 0.9078, Val Loss: 0.0135, Val Accuracy: 0.4399\n",
      "Epoch 118/200, Train Loss: 0.0009, Train Accuracy: 0.9157, Val Loss: 0.0137, Val Accuracy: 0.4174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 91\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self, criterion)\u001b[0m\n\u001b[1;32m     88\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 91\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(val_dataloader, criterion)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[11], line 51\u001b[0m, in \u001b[0;36mModelTrainer.train_one_epoch\u001b[0;34m(self, dataloader, criterion, max_grad_norm)\u001b[0m\n\u001b[1;32m     48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(audio, vision, text_bert)\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MultimodalClassifier(\n",
    "    audio_dim=74,\n",
    "    vision_dim=35,\n",
    "    text_dim=3,  # Assuming each time step of BERT features has 3 dimensions\n",
    "    hidden_dim=128,\n",
    "    num_classes=3\n",
    ")\n",
    "\n",
    "# Define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming train_dataset and valid_dataset are already defined and loaded\n",
    "# Initialize the ModelTrainer\n",
    "trainer = ModelTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=valid_dataset,\n",
    "    model_name='multimodal_classifier',\n",
    "    epochs=200,  # Set the number of epochs according to your needs\n",
    "    save_interval=2,\n",
    "    lr=1e-3,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d6ac1-ab14-4ce3-ba72-4245de5937a1",
   "metadata": {},
   "source": [
    "# Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d91ac06-7da2-4c33-8e6b-17f06237c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTransformer(nn.Module):\n",
    "    def __init__(self, audio_feature_dim, vision_feature_dim, text_feature_dim, num_classes, hidden_dim=256, num_heads=4, num_layers=6, dropout_rate=0.3):\n",
    "        super(MultimodalTransformer, self).__init__()\n",
    "        \n",
    "        # Define encoders for each modality\n",
    "        self.audio_encoder = nn.Linear(audio_feature_dim, hidden_dim)\n",
    "        self.vision_encoder = nn.Linear(vision_feature_dim, hidden_dim)\n",
    "        self.text_encoder = nn.Linear(text_feature_dim, hidden_dim)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classifier layer\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, audio, vision, text_bert):\n",
    "        # Average the audio and vision features over time\n",
    "        audio_feature = audio.mean(dim=1)\n",
    "        vision_feature = vision.mean(dim=1)\n",
    "        \n",
    "        # Flatten the text features\n",
    "        text_feature = text_bert.view(text_bert.size(0), -1)\n",
    "        \n",
    "        # Encode and apply dropout to the features\n",
    "        audio_encoded = self.dropout(self.audio_encoder(audio_feature))\n",
    "        vision_encoded = self.dropout(self.vision_encoder(vision_feature))\n",
    "        text_encoded = self.dropout(self.text_encoder(text_feature))\n",
    "        \n",
    "        # Combine the features\n",
    "        combined_features = audio_encoded + vision_encoded + text_encoded\n",
    "        \n",
    "        # Pass the combined features through the transformer encoder\n",
    "        transformer_output = self.transformer_encoder(combined_features.unsqueeze(1))\n",
    "        pooled_output = transformer_output.mean(dim=1)\n",
    "        \n",
    "        # Apply dropout before classification\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(pooled_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda662fb-18b9-4167-97ee-2607f7649e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found, starting from scratch.\n",
      "Epoch 1/100, Train Loss: 0.0043, Train Accuracy: 0.4610, Val Loss: 0.0043, Val Accuracy: 0.5131\n",
      "Epoch 2/100, Train Loss: 0.0041, Train Accuracy: 0.4891, Val Loss: 0.0044, Val Accuracy: 0.5110\n",
      "Epoch 3/100, Train Loss: 0.0041, Train Accuracy: 0.4993, Val Loss: 0.0043, Val Accuracy: 0.5110\n",
      "Epoch 4/100, Train Loss: 0.0040, Train Accuracy: 0.5017, Val Loss: 0.0043, Val Accuracy: 0.5158\n",
      "Epoch 5/100, Train Loss: 0.0040, Train Accuracy: 0.5057, Val Loss: 0.0043, Val Accuracy: 0.5190\n",
      "Epoch 6/100, Train Loss: 0.0040, Train Accuracy: 0.5075, Val Loss: 0.0043, Val Accuracy: 0.5115\n",
      "Epoch 7/100, Train Loss: 0.0040, Train Accuracy: 0.5063, Val Loss: 0.0044, Val Accuracy: 0.5158\n",
      "Epoch 8/100, Train Loss: 0.0040, Train Accuracy: 0.5082, Val Loss: 0.0044, Val Accuracy: 0.5174\n",
      "Epoch 9/100, Train Loss: 0.0040, Train Accuracy: 0.5045, Val Loss: 0.0043, Val Accuracy: 0.5184\n",
      "Epoch 10/100, Train Loss: 0.0040, Train Accuracy: 0.5069, Val Loss: 0.0043, Val Accuracy: 0.5094\n",
      "Epoch 11/100, Train Loss: 0.0040, Train Accuracy: 0.5070, Val Loss: 0.0044, Val Accuracy: 0.5190\n",
      "Epoch 12/100, Train Loss: 0.0040, Train Accuracy: 0.5080, Val Loss: 0.0043, Val Accuracy: 0.5200\n",
      "Epoch 13/100, Train Loss: 0.0040, Train Accuracy: 0.5077, Val Loss: 0.0043, Val Accuracy: 0.5216\n",
      "Epoch 14/100, Train Loss: 0.0040, Train Accuracy: 0.5073, Val Loss: 0.0043, Val Accuracy: 0.5195\n",
      "Epoch 15/100, Train Loss: 0.0040, Train Accuracy: 0.5097, Val Loss: 0.0043, Val Accuracy: 0.5195\n",
      "Epoch 16/100, Train Loss: 0.0040, Train Accuracy: 0.5088, Val Loss: 0.0043, Val Accuracy: 0.5216\n",
      "Epoch 17/100, Train Loss: 0.0040, Train Accuracy: 0.5068, Val Loss: 0.0043, Val Accuracy: 0.5216\n",
      "Epoch 18/100, Train Loss: 0.0040, Train Accuracy: 0.5077, Val Loss: 0.0043, Val Accuracy: 0.5200\n",
      "Epoch 19/100, Train Loss: 0.0040, Train Accuracy: 0.5121, Val Loss: 0.0044, Val Accuracy: 0.5211\n",
      "Epoch 20/100, Train Loss: 0.0040, Train Accuracy: 0.5091, Val Loss: 0.0043, Val Accuracy: 0.5211\n",
      "Epoch 21/100, Train Loss: 0.0040, Train Accuracy: 0.5104, Val Loss: 0.0044, Val Accuracy: 0.5200\n",
      "Epoch 22/100, Train Loss: 0.0040, Train Accuracy: 0.5105, Val Loss: 0.0043, Val Accuracy: 0.5200\n",
      "Epoch 23/100, Train Loss: 0.0040, Train Accuracy: 0.5095, Val Loss: 0.0043, Val Accuracy: 0.5179\n",
      "Epoch 24/100, Train Loss: 0.0040, Train Accuracy: 0.5103, Val Loss: 0.0043, Val Accuracy: 0.5238\n",
      "Epoch 25/100, Train Loss: 0.0040, Train Accuracy: 0.5115, Val Loss: 0.0043, Val Accuracy: 0.5211\n",
      "Epoch 26/100, Train Loss: 0.0040, Train Accuracy: 0.5105, Val Loss: 0.0043, Val Accuracy: 0.5200\n",
      "Epoch 27/100, Train Loss: 0.0040, Train Accuracy: 0.5135, Val Loss: 0.0043, Val Accuracy: 0.5168\n",
      "Epoch 28/100, Train Loss: 0.0040, Train Accuracy: 0.5119, Val Loss: 0.0043, Val Accuracy: 0.5227\n",
      "Epoch 29/100, Train Loss: 0.0040, Train Accuracy: 0.5142, Val Loss: 0.0044, Val Accuracy: 0.5200\n",
      "Epoch 30/100, Train Loss: 0.0040, Train Accuracy: 0.5135, Val Loss: 0.0044, Val Accuracy: 0.5190\n",
      "Epoch 31/100, Train Loss: 0.0040, Train Accuracy: 0.5124, Val Loss: 0.0043, Val Accuracy: 0.5136\n",
      "Epoch 32/100, Train Loss: 0.0040, Train Accuracy: 0.5137, Val Loss: 0.0043, Val Accuracy: 0.5259\n",
      "Epoch 33/100, Train Loss: 0.0040, Train Accuracy: 0.5130, Val Loss: 0.0044, Val Accuracy: 0.5222\n",
      "Epoch 34/100, Train Loss: 0.0040, Train Accuracy: 0.5129, Val Loss: 0.0044, Val Accuracy: 0.5206\n",
      "Epoch 35/100, Train Loss: 0.0040, Train Accuracy: 0.5138, Val Loss: 0.0043, Val Accuracy: 0.5190\n",
      "Epoch 36/100, Train Loss: 0.0040, Train Accuracy: 0.5157, Val Loss: 0.0044, Val Accuracy: 0.5200\n",
      "Epoch 37/100, Train Loss: 0.0040, Train Accuracy: 0.5154, Val Loss: 0.0044, Val Accuracy: 0.5190\n",
      "Epoch 38/100, Train Loss: 0.0040, Train Accuracy: 0.5170, Val Loss: 0.0043, Val Accuracy: 0.5195\n",
      "Epoch 39/100, Train Loss: 0.0040, Train Accuracy: 0.5129, Val Loss: 0.0043, Val Accuracy: 0.5206\n",
      "Epoch 40/100, Train Loss: 0.0040, Train Accuracy: 0.5169, Val Loss: 0.0044, Val Accuracy: 0.5195\n",
      "Epoch 41/100, Train Loss: 0.0040, Train Accuracy: 0.5151, Val Loss: 0.0044, Val Accuracy: 0.5211\n",
      "Epoch 42/100, Train Loss: 0.0040, Train Accuracy: 0.5169, Val Loss: 0.0044, Val Accuracy: 0.5216\n",
      "Epoch 43/100, Train Loss: 0.0040, Train Accuracy: 0.5158, Val Loss: 0.0043, Val Accuracy: 0.5179\n",
      "Epoch 44/100, Train Loss: 0.0040, Train Accuracy: 0.5176, Val Loss: 0.0044, Val Accuracy: 0.5190\n",
      "Epoch 45/100, Train Loss: 0.0040, Train Accuracy: 0.5174, Val Loss: 0.0044, Val Accuracy: 0.5174\n",
      "Epoch 46/100, Train Loss: 0.0040, Train Accuracy: 0.5171, Val Loss: 0.0044, Val Accuracy: 0.5190\n",
      "Epoch 47/100, Train Loss: 0.0040, Train Accuracy: 0.5153, Val Loss: 0.0044, Val Accuracy: 0.5190\n",
      "Epoch 48/100, Train Loss: 0.0039, Train Accuracy: 0.5164, Val Loss: 0.0044, Val Accuracy: 0.5179\n",
      "Epoch 49/100, Train Loss: 0.0039, Train Accuracy: 0.5157, Val Loss: 0.0044, Val Accuracy: 0.5190\n",
      "Epoch 50/100, Train Loss: 0.0039, Train Accuracy: 0.5200, Val Loss: 0.0044, Val Accuracy: 0.5163\n",
      "Epoch 51/100, Train Loss: 0.0039, Train Accuracy: 0.5160, Val Loss: 0.0044, Val Accuracy: 0.5206\n",
      "Epoch 52/100, Train Loss: 0.0039, Train Accuracy: 0.5178, Val Loss: 0.0044, Val Accuracy: 0.5115\n",
      "Epoch 53/100, Train Loss: 0.0039, Train Accuracy: 0.5183, Val Loss: 0.0044, Val Accuracy: 0.5174\n",
      "Epoch 54/100, Train Loss: 0.0039, Train Accuracy: 0.5208, Val Loss: 0.0044, Val Accuracy: 0.5168\n",
      "Epoch 55/100, Train Loss: 0.0039, Train Accuracy: 0.5198, Val Loss: 0.0044, Val Accuracy: 0.5152\n",
      "Epoch 56/100, Train Loss: 0.0039, Train Accuracy: 0.5165, Val Loss: 0.0044, Val Accuracy: 0.5142\n",
      "Epoch 57/100, Train Loss: 0.0039, Train Accuracy: 0.5141, Val Loss: 0.0044, Val Accuracy: 0.5142\n",
      "Epoch 58/100, Train Loss: 0.0039, Train Accuracy: 0.5194, Val Loss: 0.0044, Val Accuracy: 0.5179\n",
      "Epoch 59/100, Train Loss: 0.0039, Train Accuracy: 0.5199, Val Loss: 0.0044, Val Accuracy: 0.5147\n",
      "Epoch 60/100, Train Loss: 0.0039, Train Accuracy: 0.5193, Val Loss: 0.0044, Val Accuracy: 0.5099\n",
      "Epoch 61/100, Train Loss: 0.0039, Train Accuracy: 0.5190, Val Loss: 0.0044, Val Accuracy: 0.5152\n",
      "Epoch 62/100, Train Loss: 0.0039, Train Accuracy: 0.5228, Val Loss: 0.0044, Val Accuracy: 0.5168\n",
      "Epoch 63/100, Train Loss: 0.0039, Train Accuracy: 0.5196, Val Loss: 0.0045, Val Accuracy: 0.5168\n",
      "Epoch 64/100, Train Loss: 0.0039, Train Accuracy: 0.5200, Val Loss: 0.0044, Val Accuracy: 0.5110\n",
      "Epoch 65/100, Train Loss: 0.0039, Train Accuracy: 0.5222, Val Loss: 0.0044, Val Accuracy: 0.5126\n",
      "Epoch 66/100, Train Loss: 0.0039, Train Accuracy: 0.5231, Val Loss: 0.0045, Val Accuracy: 0.5126\n",
      "Epoch 67/100, Train Loss: 0.0039, Train Accuracy: 0.5244, Val Loss: 0.0044, Val Accuracy: 0.5067\n",
      "Epoch 68/100, Train Loss: 0.0039, Train Accuracy: 0.5224, Val Loss: 0.0044, Val Accuracy: 0.5152\n",
      "Epoch 69/100, Train Loss: 0.0039, Train Accuracy: 0.5189, Val Loss: 0.0045, Val Accuracy: 0.5029\n",
      "Epoch 70/100, Train Loss: 0.0039, Train Accuracy: 0.5252, Val Loss: 0.0044, Val Accuracy: 0.5158\n",
      "Epoch 71/100, Train Loss: 0.0039, Train Accuracy: 0.5198, Val Loss: 0.0045, Val Accuracy: 0.5136\n",
      "Epoch 72/100, Train Loss: 0.0039, Train Accuracy: 0.5242, Val Loss: 0.0046, Val Accuracy: 0.4912\n",
      "Epoch 73/100, Train Loss: 0.0039, Train Accuracy: 0.5243, Val Loss: 0.0044, Val Accuracy: 0.5136\n",
      "Epoch 74/100, Train Loss: 0.0039, Train Accuracy: 0.5225, Val Loss: 0.0045, Val Accuracy: 0.5168\n",
      "Epoch 75/100, Train Loss: 0.0039, Train Accuracy: 0.5244, Val Loss: 0.0045, Val Accuracy: 0.5142\n",
      "Epoch 76/100, Train Loss: 0.0039, Train Accuracy: 0.5229, Val Loss: 0.0045, Val Accuracy: 0.5077\n",
      "Epoch 77/100, Train Loss: 0.0039, Train Accuracy: 0.5241, Val Loss: 0.0045, Val Accuracy: 0.5136\n",
      "Epoch 78/100, Train Loss: 0.0039, Train Accuracy: 0.5261, Val Loss: 0.0045, Val Accuracy: 0.5110\n",
      "Epoch 79/100, Train Loss: 0.0039, Train Accuracy: 0.5246, Val Loss: 0.0045, Val Accuracy: 0.5083\n",
      "Epoch 80/100, Train Loss: 0.0039, Train Accuracy: 0.5255, Val Loss: 0.0045, Val Accuracy: 0.5120\n"
     ]
    }
   ],
   "source": [
    "model = MultimodalTransformer(\n",
    "    audio_feature_dim=74,  # from Audio tensor shape [64, 50, 74]\n",
    "    vision_feature_dim=35,  # from Vision tensor shape [64, 50, 35]\n",
    "    text_feature_dim=3*50,  # from Text BERT tensor shape [64, 3, 50]\n",
    "    num_classes=3  # for the classification task\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming train_dataset and valid_dataset are defined and loaded\n",
    "trainer = ModelTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=valid_dataset,\n",
    "    model_name='multimodal_transformer5',\n",
    "    epochs=100,  # Number of training epochs\n",
    "    save_interval=10,  # Interval for saving the model checkpoint\n",
    "    lr=1e-4,  # Learning rate\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'  # Use CUDA if available\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b6925-c34a-4835-a9ba-01c2d5049783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
