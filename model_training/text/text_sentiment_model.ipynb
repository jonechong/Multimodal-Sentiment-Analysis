{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filename, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.max_len = max_len\n",
    "\n",
    "        with h5py.File(filename, 'r') as hdf:\n",
    "            for group in hdf.keys():\n",
    "                text = hdf[group].attrs['text']\n",
    "                label = int(hdf[group].attrs['label'])\n",
    "                \n",
    "                encoding = self.tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_len,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt',\n",
    "                )\n",
    "                \n",
    "                self.data.append({\n",
    "                    'input_ids': encoding['input_ids'].squeeze(),\n",
    "                    'attention_mask': encoding['attention_mask'].squeeze()\n",
    "                })\n",
    "                self.labels.append(label)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "            **self.data[item],\n",
    "            'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "def stratified_split(dataset, test_size=0.2, val_size=0.1, random_seed=None):\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    labels = np.array(dataset.labels)\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    class_indices = [np.where(labels == i)[0] for i in unique_classes]\n",
    "    \n",
    "    test_split_size = (class_counts * test_size).astype(int)\n",
    "    val_split_size = (class_counts * val_size).astype(int)\n",
    "    \n",
    "    train_indices, test_indices, val_indices = [], [], []\n",
    "    for class_idx, class_split_test, class_split_val in zip(class_indices, test_split_size, val_split_size):\n",
    "        class_indices_perm = np.random.permutation(class_idx)\n",
    "        class_test_indices = class_indices_perm[:class_split_test]\n",
    "        class_val_indices = class_indices_perm[class_split_test:class_split_test + class_split_val]\n",
    "        class_train_indices = class_indices_perm[class_split_test + class_split_val:]\n",
    "        \n",
    "        train_indices.extend(class_train_indices)\n",
    "        test_indices.extend(class_test_indices)\n",
    "        val_indices.extend(class_val_indices)\n",
    "    \n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch in tqdm(dataloader, total=len(dataloader), desc=\"Training\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        model.zero_grad()  \n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_examples = 0, 0, 0\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=len(dataloader), desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_correct += (preds == batch['labels']).sum().item()\n",
    "            total_examples += batch['labels'].size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_examples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader/Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, file_path=\"model_checkpoints\"):\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    \n",
    "    checkpoint_path = os.path.join(file_path, f\"model_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, file_path):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {file_path}, epoch {epoch}\")\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "filename = \"../text_labels.h5\"  \n",
    "max_len = 128 \n",
    "dataset = CustomDataset(filename=filename, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "n_classes = 3  \n",
    "model = SentimentClassifier(n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = int(0.1 * len(dataset))\n",
    "# test_size = len(dataset) - train_size - val_size\n",
    "test_size = 0.1\n",
    "val_size = 0.1\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = stratified_split(dataset, test_size=test_size, val_size=val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Load checkpoint (ignore if no checkpoint exists)\n",
    "checkpoint_file = \"model_checkpoints/model_epoch_XX.pth\"\n",
    "\n",
    "if os.path.isfile(checkpoint_file):\n",
    "    starting_epoch = load_checkpoint(model, optimizer, checkpoint_file)\n",
    "else:\n",
    "    starting_epoch = 0\n",
    "\n",
    "\n",
    "# Training and validation loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(starting_epoch, epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    print(f'Training loss: {train_loss}')\n",
    "    train_losses.append(train_loss)  \n",
    "\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader, device)\n",
    "    print(f'Validation loss: {val_loss}, Accuracy: {val_accuracy}')\n",
    "    val_losses.append(val_loss)  \n",
    "\n",
    "    save_checkpoint(model, optimizer, epoch, file_path=\"model_checkpoints\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "n = len(train_losses)\n",
    "plt.plot(range(1, n+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, n+1), val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = \"model_checkpoints/model_epoch_0.pth\"\n",
    "load_checkpoint(model, optimizer, checkpoint_file)\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, device)\n",
    "print(f'Test loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUTD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
